{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA865 - Lecture 04.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "qvTecanMe46Q",
        "u3GUWHlBKnkr",
        "0tSEwOAuLPkh",
        "oNOWs_wyLV8I",
        "lzdV9m1YOGJE",
        "2d0cysIBlFpX",
        "liYk79MN9VOT",
        "QwPNQU3iP9NX",
        "icDIyFyTKnmr",
        "CMtzYY15UOkE",
        "I8ANw4ehU6c_",
        "0XDB_Tt0Knnn",
        "103xl_8p-XFh",
        "V7Zd1WrpOH4j",
        "tifHnbMvwzBo",
        "Ax9jhXIYw58Y",
        "l1D7xOU6KnoG",
        "Lpi_22UpxSRK",
        "6rq5fAWOL_Jx",
        "yhWp-ZnNxuHm",
        "n7TVWX_x7SXx",
        "ubArpkthX9bh",
        "7Ocdq0qNYDjS",
        "ZhowOewtjZW2",
        "1NJAgVOhP3Q9",
        "zlk1CKbnWFTR",
        "q866cQsKfHkp",
        "m6BcgofkhVTb",
        "7EVoKfb8WJVU",
        "ow--Rj5v0v7B",
        "A4qz9NtLhQfP",
        "FZ7BhyvXyiHC",
        "oj4dVuyCNeIU",
        "E65VEncJug2E",
        "qEezkB-EdNL7",
        "dQhHM6edeSHv",
        "P-P5rnFFhA07",
        "AtrGe_7viPZs",
        "Q-zOhC1ouqdG",
        "xMmqhisRPoTG",
        "HnEyGAVg2z87",
        "jReiPWlGAzLG",
        "m5LPCyGRczAR",
        "gOKXMJBCU_qF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/BA865/blob/master/BA865_Lecture_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvTecanMe46Q",
        "colab_type": "text"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxE4uqwMfpUg",
        "colab_type": "text"
      },
      "source": [
        "Below is some code that we will make use of to make our lives easier.  Imports and functions are usually introduced throughout this notebook as needed. But sometimes you need to jump around and run things out of order. This cell below makes it easier to do that by running it before you begin.\n",
        "\n",
        "**You don't need to look at this code now.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVOikh64e3qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports for modules we will use:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import feather\n",
        "import datetime\n",
        "import pandas_datareader.data as web\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 20, 'figure.figsize': (20, 10)}) # set font and plot size\n",
        "\n",
        "\n",
        "# Some code to make displaying multiple dataframes side by side better\n",
        "class display(object):\n",
        "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
        "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
        "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
        "    </div>\"\"\"\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
        "                         for a in self.args)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
        "                           for a in self.args)\n",
        "\n",
        "\n",
        "# Some code to generate example dataframes\n",
        "def make_df(cols, ind):\n",
        "    \"\"\"Quickly make a DataFrame\"\"\"\n",
        "    data = {c: [str(c) + str(i) for i in ind]\n",
        "            for c in cols}\n",
        "    return pd.DataFrame(data, ind)\n",
        "\n",
        "# Load imdbb datasets that we'll use\n",
        "imdbFile = 'https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/IMDB-Movie-Data.csv'\n",
        "movies_df = pd.read_csv(imdbFile, index_col=\"Title\")\n",
        "movies_df.columns = ['rank', 'genre', 'description', 'director', 'actors', 'year', 'runtime', \n",
        "                     'rating', 'votes', 'revenue_millions', 'metascore']\n",
        "\n",
        "planets = sns.load_dataset('planets')\n",
        "\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "stDate = datetime.datetime(2020,1,1)\n",
        "enDate = datetime.datetime(2020,1,7)\n",
        "stocks = [\"AMZN\",\"MSFT\",\"NVDA\",\"NTDOY\", \"AAPL\"]\n",
        "stocks_df = pd.concat([ web.DataReader(st,'yahoo',stDate,enDate).assign(Stock=st)[['Stock','Open','Close']] for st in stocks ]) # read this line from the inside out\n",
        "stocks_1day_df = stocks_df.reset_index()\n",
        "stocks_1day_df = stocks_1day_df[stocks_1day_df.Date==enDate].reset_index().drop(columns=['Date','index'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3GUWHlBKnkr",
        "colab_type": "text"
      },
      "source": [
        "# Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcswF_j4Knkv",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1QblXB3vwRh63bXvi7pM_yk4PsG6MMw1i' width=500>\n",
        "\n",
        "The *pandas* package is the most important tool at the disposal of Data Scientists and Analysts working in Python today. The powerful machine learning and glamorous visualization tools may get all the attention, but pandas is the backbone of most data projects. \n",
        "\n",
        ">\\[*pandas*\\] is derived from the term \"**pan**el **da**ta\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. — [Wikipedia](https://en.wikipedia.org/wiki/Pandas_%28software%29)\n",
        "\n",
        "If you're thinking about data science as a career, then it is imperative that one of the first things you do is learn pandas. In this post, we will go over the essential bits of information about pandas, including how to install it, its uses, and how it works with other common Python data analysis packages such as **matplotlib** and **sci-kit learn**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G62x56FfKnk1",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Hsr__ImXc8TRS44Od6jrRzLl1y3Y3cLL\" width=500px />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tSEwOAuLPkh",
        "colab_type": "text"
      },
      "source": [
        "# What's Pandas for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPn2Ny4hKnk3",
        "colab_type": "text"
      },
      "source": [
        "Pandas has so many uses that it might make sense to list the things it can't do instead of what it can do. \n",
        "\n",
        "This tool is essentially your data’s home. Through pandas, you get acquainted with your data by cleaning, transforming, and analyzing it. \n",
        "\n",
        "For example, say you want to explore a dataset stored in a CSV on your computer. Pandas will extract the data from that CSV into a DataFrame — a table, basically — then let you do things like:\n",
        "\n",
        "- Calculate statistics and answer questions about the data, like\n",
        "\n",
        "\n",
        "    - What's the average, median, max, or min of each column? \n",
        "    - Does column A correlate with column B?\n",
        "    - What does the distribution of data in column C look like?\n",
        "\n",
        "\n",
        "- Clean the data by doing things like removing missing values and filtering rows or columns by some criteria\n",
        "\n",
        "\n",
        "- Visualize the data with help from Matplotlib. Plot bars, lines, histograms, bubbles, and more. \n",
        "\n",
        "\n",
        "- Store the cleaned, transformed data back into a CSV, other file or database\n",
        "\n",
        "\n",
        "Before you jump into the modeling or the complex visualizations you need to have a good understanding of the nature of your dataset and pandas is the best avenue through which to do that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNOWs_wyLV8I",
        "colab_type": "text"
      },
      "source": [
        "# How does pandas fit into the data science toolkit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq1tpQIdKnk7",
        "colab_type": "text"
      },
      "source": [
        "Not only is the pandas library a central component of the data science toolkit but it is used in conjunction with other libraries in that collection. \n",
        "\n",
        "Pandas is built on top of the **NumPy** package, meaning a lot of the structure of NumPy is used or replicated in Pandas. Data in pandas is often used to feed statistical analysis in **SciPy**, plotting functions from **Matplotlib**, and machine learning algorithms in **Scikit-learn**.\n",
        "\n",
        "Before we do anything else, we have to import pandas. For brevity we'll import is as ``pd`` so we don't have to type ``pandas.`` everywhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntRm7WB4Knlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdY9ynw8Knl0",
        "colab_type": "text"
      },
      "source": [
        "Now to the basic components of pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzdV9m1YOGJE",
        "colab_type": "text"
      },
      "source": [
        "# Core components of pandas: Series and DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgp8GqfyKnl3",
        "colab_type": "text"
      },
      "source": [
        "The primary two components of pandas are the `Series` and `DataFrame`. \n",
        "\n",
        "A `Series` is essentially a column, and a `DataFrame` is a multi-dimensional table made up of a collection of Series. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1GNUYVXJ_dZK94GBJZWLbvAD2xOC0K342\" width=600px />\n",
        "\n",
        "DataFrames and Series are quite similar in that many operations that you can do with one you can do with the other, such as filling in null values and calculating the mean.\n",
        "\n",
        "You'll see how these components work when we start working with data below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0cysIBlFpX",
        "colab_type": "text"
      },
      "source": [
        "## Creating DataFrames from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reaSxs_AKnl9",
        "colab_type": "text"
      },
      "source": [
        "Creating DataFrames right in Python is good to know and quite useful when testing new methods and functions you find in the pandas docs.\n",
        "\n",
        "There are *many* ways to create a DataFrame from scratch, but a great option is to just use a simple `dict` of `list`s. \n",
        "\n",
        "Let's say we have a fruit stand that sells apples and oranges. We want to have a column for each fruit and a row for each customer purchase. To organize this as a dictionary for pandas we could do something like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg2uELI_Knl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {\n",
        "    'apples': [3, 2, 0, 1], \n",
        "    'oranges': [0, 3, 7, 2]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywLmVE1GKnmM",
        "colab_type": "text"
      },
      "source": [
        "And then pass it to the pandas DataFrame constructor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSyM0i38KnmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purchases = pd.DataFrame(data)\n",
        "\n",
        "purchases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmJw3BhpKnma",
        "colab_type": "text"
      },
      "source": [
        "**How did that work?**\n",
        "\n",
        "Each *(key, value)* item in `data` corresponds to a *column* in the resulting DataFrame.\n",
        "\n",
        "The **Index** of this DataFrame was given to us on creation as the numbers 0-3, but we could also create our own when we initialize the DataFrame. \n",
        "\n",
        "Let's have customer names as our index: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oCR9N3JKnmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purchases = pd.DataFrame(data, index=['June', 'Robert', 'Lily', 'David'])\n",
        "\n",
        "purchases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE08lxOgKnmj",
        "colab_type": "text"
      },
      "source": [
        "So now we could **loc**ate a customer's order by using their name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKnI37ugKnml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purchases.loc['June']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0sv0xtKnmp",
        "colab_type": "text"
      },
      "source": [
        "There's more on locating and extracting data from the DataFrame later, but now you should be able to create a DataFrame with any random data to learn on.\n",
        "\n",
        "Let's move on to some quick methods for creating DataFrames from various other sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liYk79MN9VOT",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Create a dataframe of characters from your favorite movie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed3wIZHU9i4p",
        "colab_type": "text"
      },
      "source": [
        "Create a dataframe from scratch of characters from your favorite movie\n",
        "\n",
        "Columns could include:\n",
        "- actor name\n",
        "- character gender\n",
        "- A boolean that is True if the character is a villain\n",
        "- ... and any other column you'd like to add\n",
        "\n",
        "Make the index of the dataframe the character name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MGXE-wS9yqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create your dataframe here\n",
        "\n",
        "# Create your dataframe here\n",
        "data = {\n",
        "    'Actress': [ 'Audrey Hepburn', 'George Peppard'],\n",
        "    'Gender': ['Female','Male'],\n",
        "    'Villain': [False,False],\n",
        "    'CharacterName':['Holly Golightly','Paul Varjak']\n",
        "}\n",
        "movie_BaT = pd.DataFrame(data)\n",
        "movie_BaT\n",
        "#movie_BaT.set_index('CharacterName',inplace=True)\n",
        "movie_BaT = movie_BaT.set_index('CharacterName')\n",
        "movie_BaT\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwPNQU3iP9NX",
        "colab_type": "text"
      },
      "source": [
        "# How to read data with pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1guXRvAKnmq",
        "colab_type": "text"
      },
      "source": [
        "It’s quite simple to load data from various file formats into a DataFrame. In the following examples we'll keep using our apples and oranges data, but this time, read it from various files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icDIyFyTKnmr",
        "colab_type": "text"
      },
      "source": [
        "## Reading data from CSVs\n",
        "\n",
        "With CSV files all you need is a single line to load in the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eaqx8_-Knms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purchasesFile='https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/purchases.csv'\n",
        "df = pd.read_csv(purchasesFile) \n",
        "# If we were running this from a local machine with purchases.csv already present in the current directory\n",
        "#  we would have instead typed pd.read_csv('purchases.csv')\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbsKtWXcKnmv",
        "colab_type": "text"
      },
      "source": [
        "CSVs don't have indexes like our DataFrames, so all we need to do is just designate the `index_col` when reading:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx-wXS60Knmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(purchasesFile, index_col=0) # Set the index to be the 0th column\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVnGGSWeKnm0",
        "colab_type": "text"
      },
      "source": [
        "Here we're setting the index to be column zero.\n",
        "\n",
        "You'll find that most CSVs won't ever have an index column and so usually you don't have to worry about this step.\n",
        "\n",
        "It's also possible to set an index on a dataframe after its been created with `df.set_index('someIndex')`. You can read more about it [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMtzYY15UOkE",
        "colab_type": "text"
      },
      "source": [
        "## Reading data from JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgVVZZ54UIPk",
        "colab_type": "text"
      },
      "source": [
        "If you have a JSON file — which is essentially a stored Python `dict` — pandas can read this just as easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkDbLdspKnm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purchasesJSONFile='https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/purchases.json'\n",
        "df = pd.read_json(purchasesJSONFile)\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzjcnWMcKnnF",
        "colab_type": "text"
      },
      "source": [
        "Notice this time our index came with us correctly since using JSON allowed indexes to work through nesting. You can open the `purchases.json` file by pasting the purchasesJSONFile link above into an empty tab on your browser so you can see the file structure.\n",
        "\n",
        "Pandas will try to figure out how to create a DataFrame by analyzing structure of your JSON, and sometimes it doesn't get it right. Often you'll need to set the `orient` keyword argument depending on the structure, so check out [read_json docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) about that argument to see which orientation you're using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8ANw4ehU6c_",
        "colab_type": "text"
      },
      "source": [
        "## Reading data from a SQL database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNCnCkvzU7vb",
        "colab_type": "text"
      },
      "source": [
        "If you’re working with data from a SQL database you would need to first establish a connection using an appropriate Python library, then pass a query to pandas. \n",
        "\n",
        "I'm not going to demonstrate this with code here, as it requires installing a mysql module in Google Colab and having a database running that it can connect to.  However, I'll show you some example code to give you an idea:\n",
        "```\n",
        "import pymysql\n",
        "\n",
        "# Connect to the database:\n",
        "con = pymysql.connect(host='localhost',user='user',password='password',db='db')\n",
        "\n",
        "# read the results of a query into a dataframe:\n",
        "df = pd.read_sql_query(\"SELECT * FROM purchases\", con)\n",
        "``` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XDB_Tt0Knnn",
        "colab_type": "text"
      },
      "source": [
        "## Converting back to a CSV, JSON, or SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKn_wXbnX0sa",
        "colab_type": "text"
      },
      "source": [
        "So after extensive work on cleaning your data, you’re now ready to save it as a file of your choice. Similar to the ways we read in data, pandas provides intuitive commands to save it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0355CTcyKnnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('new_purchases.csv')\n",
        "\n",
        "df.to_json('new_purchases.json')\n",
        "\n",
        "#df.to_sql('new_purchases', con) # This line is commented out, because we didn't establish a connection to a dB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvS3ZHVEOCJN",
        "colab_type": "text"
      },
      "source": [
        "Check out the local Files in Google Colab ( little right arrow under \"+Code\") to verify that these files have indeed been created on our local machine.\n",
        "\n",
        "When we save JSON and CSV files, all we have to input into those functions is our desired filename with the appropriate file extension. With SQL, we’re not creating a new file but instead inserting a new table into the database using our `con` variable from before.\n",
        "\n",
        "Let's move on to importing some real-world data and detailing a few of the operations you'll be using a lot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103xl_8p-XFh",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Write your Movie Character dataframe out to a csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JunbhtjPKzR1",
        "colab_type": "text"
      },
      "source": [
        "Using the movie character dataframe that you created earlier, write it out to the csv file \"moviechars_df.csv\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "827y9-Kb-eRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwxc_j9T-jAT",
        "colab_type": "text"
      },
      "source": [
        "Now, open the file by using Google Colab's interface, to verify that it makes wrote correctly and makes sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Zd1WrpOH4j",
        "colab_type": "text"
      },
      "source": [
        "# Basic DataFrame operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "466GPuL1Knnt",
        "colab_type": "text"
      },
      "source": [
        "DataFrames possess hundreds of methods and other operations that are crucial to any analysis. As a beginner, you should know the operations that perform simple transformations of your data and those that provide fundamental statistical analysis.\n",
        "\n",
        "Let's load in the IMDB movies dataset to begin:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDSvCLJgKnnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdbFile = 'https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/IMDB-Movie-Data.csv'\n",
        "movies_df = pd.read_csv(imdbFile, index_col=\"Title\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEq9JPO2Knny",
        "colab_type": "text"
      },
      "source": [
        "We're loading this dataset from a CSV and designating the movie titles to be our index.\n",
        "\n",
        "Because we did this, Title won't be a column name in our DataFrame (it's the index). If for some reason, we wanted to make an index a column again, we could use the `reset_index()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk9Q6d5NP-_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tifHnbMvwzBo",
        "colab_type": "text"
      },
      "source": [
        "## Viewing your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA9FvcXmKnnz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The first thing to do when opening a new dataset is print out a few rows to keep as a visual reference. We accomplish this with `.head()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyOwvBP3Knn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1LOrsfoKnn6",
        "colab_type": "text"
      },
      "source": [
        "`.head()` outputs the **first** five rows of your DataFrame by default, but we could also pass a number as well: `movies_df.head(10)` would output the top ten rows, for example. \n",
        "\n",
        "To see the **last** five rows use `.tail()`. `tail()` also accepts a number, and in this case we printing the bottom two rows.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njt4MU_KKnn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.tail(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H4KKH9oKnn9",
        "colab_type": "text"
      },
      "source": [
        "Typically when we load in a dataset, we like to view the first five or so rows to see what's under the hood. Here we can see the names of each column, the index, and examples of values in each row.\n",
        "\n",
        "You'll notice that the index in our DataFrame is the *Title* column, which you can tell by how the word *Title* is slightly lower than the rest of the columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax9jhXIYw58Y",
        "colab_type": "text"
      },
      "source": [
        "## Getting info about your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWiuR9AgKnn_",
        "colab_type": "text"
      },
      "source": [
        "`.info()` should be one of the very first commands you run after loading your data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UALXdwD0Knn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etz2Uz7zKnoC",
        "colab_type": "text"
      },
      "source": [
        "`.info()` provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using. \n",
        "\n",
        "Notice in our movies dataset we have some obvious missing values in the `Revenue` and `Metascore` columns. We'll look at how to handle those in a bit.\n",
        "\n",
        "Seeing the datatype quickly is actually quite useful. Imagine you just imported some JSON and the integers were recorded as strings. You go to do some arithmetic and find an \"unsupported operand\" Exception because you can't do math with strings. Calling `.info()` will quickly point out that your column you thought was all integers are actually string objects.\n",
        "\n",
        "Another fast and useful attribute is `.shape`, which outputs just a tuple of (rows, columns):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fYMKS-yKnoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me8JRvCWKnoF",
        "colab_type": "text"
      },
      "source": [
        "Note that `.shape` has no parentheses (its a property, not a method) and is a simple tuple of format (rows, columns). So we have **1000 rows** and **11 columns** in our movies DataFrame.\n",
        "\n",
        "You'll be going to `.shape` a lot when cleaning and transforming data. For example, you might filter some rows based on some criteria and then want to know quickly how many rows were removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1D7xOU6KnoG",
        "colab_type": "text"
      },
      "source": [
        "## Handling duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORKGc1wpKnoJ",
        "colab_type": "text"
      },
      "source": [
        "This dataset does not have duplicate rows, but it is always important to verify you aren't aggregating duplicate rows. \n",
        "\n",
        "To demonstrate, let's simply just double up our movies DataFrame by appending it to itself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnyVj6MFKnoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_df = movies_df.append(movies_df)\n",
        "\n",
        "temp_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53_86AYFKnoN",
        "colab_type": "text"
      },
      "source": [
        "Using `append()` will return a copy without affecting the original DataFrame. We are capturing this copy in `temp_df` so we aren't working with the real data.\n",
        "\n",
        "Notice call `.shape` quickly proves our DataFrame rows have doubled.\n",
        "\n",
        "Now we can try dropping duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7HawSP_KnoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_df = temp_df.drop_duplicates()\n",
        "\n",
        "temp_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ET7Qe5KnoQ",
        "colab_type": "text"
      },
      "source": [
        "Just like `append()`, the `drop_duplicates()` method will also return a copy of your DataFrame, but this time with duplicates removed. Calling `.shape` confirms we're back to the 1000 rows of our original dataset.\n",
        "\n",
        "It's a little verbose to keep assigning DataFrames to the same variable like in this example. For this reason, pandas has the `inplace` keyword argument on many of its methods. Using `inplace=True` will modify the DataFrame object in place:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urOAbwJ-KnoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_df.drop_duplicates(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC3b12DCKnoV",
        "colab_type": "text"
      },
      "source": [
        "Now our `temp_df` *will* have the transformed data automatically. \n",
        "\n",
        "Another important argument for `drop_duplicates()` is `keep`, which has three possible options:\n",
        "\n",
        "* `first`: (default) Drop duplicates except for the first occurrence.\n",
        "* `last`: Drop duplicates except for the last occurrence.\n",
        "* `False`: Drop all duplicates.\n",
        "\n",
        "Since we didn't define the `keep` arugment in the previous example it was defaulted to `first`. This means that if two rows are the same pandas will drop the second row and keep the first row. Using `last` has the opposite effect: the first row is dropped.\n",
        "\n",
        "`keep`, on the other hand, will drop all duplicates. If two rows are the same then both will be dropped. Watch what happens to `temp_df`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPCmI0gmKnoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_df = movies_df.append(movies_df)  # make a new copy\n",
        "\n",
        "temp_df.drop_duplicates(inplace=True, keep=False)\n",
        "\n",
        "temp_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2lD9NAUKnoc",
        "colab_type": "text"
      },
      "source": [
        "Since all rows were duplicates, `keep=False` dropped them all resulting in zero rows being left over. If you're wondering why you would want to do this, one reason is that it allows you to locate all duplicates in your dataset. When conditional selections are shown below you'll see how to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpi_22UpxSRK",
        "colab_type": "text"
      },
      "source": [
        "## Column cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFzodE3IKnod",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Many times datasets will have verbose column names with symbols, upper and lowercase words, spaces, and typos. To make selecting data by column name easier we can spend a little time cleaning up their names.\n",
        "\n",
        "Here's how to print the column names of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKXDx5ZwKnof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXH4TKpJKnoj",
        "colab_type": "text"
      },
      "source": [
        "Not only does `.columns` come in handy if you want to rename columns by allowing for simple copy and paste, it's also useful if you need to understand why you are receiving a `Key Error` when selecting data by column.\n",
        "\n",
        "We can use the `.rename()` method to rename certain or all columns via a `dict`. We don't want parentheses, so let's rename those:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_DDiUzOKnok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.rename(columns={\n",
        "        'Runtime (Minutes)': 'Runtime', \n",
        "        'Revenue (Millions)': 'Revenue_millions'\n",
        "    }, inplace=True)\n",
        "\n",
        "\n",
        "movies_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCWK9WF4Knoq",
        "colab_type": "text"
      },
      "source": [
        "Excellent. But what if we want to lowercase all names? Instead of using `.rename()` we could also set a list of names to the columns like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtBUwBlRKnor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.columns = ['rank', 'genre', 'description', 'director', 'actors', 'year', 'runtime', \n",
        "                     'rating', 'votes', 'revenue_millions', 'metascore']\n",
        "\n",
        "\n",
        "movies_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8AijBEiKnot",
        "colab_type": "text"
      },
      "source": [
        "But that's too much work. Instead of just renaming each column manually we can do a list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8mCJC0vKnot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.columns = [col.lower() for col in movies_df]\n",
        "\n",
        "movies_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goduhka2Know",
        "colab_type": "text"
      },
      "source": [
        "`list` (and `dict`) comprehensions come in handy a lot when working with pandas and data in general.\n",
        "\n",
        "It's a good idea to lowercase, remove special characters, and replace spaces with underscores if you'll be working with a dataset for some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rq5fAWOL_Jx",
        "colab_type": "text"
      },
      "source": [
        "## How to work with missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5r6UYjIKnox",
        "colab_type": "text"
      },
      "source": [
        "When exploring data, you’ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you'll see Python's `None` or NumPy's `np.nan`, each of which are handled differently in some situations.\n",
        "\n",
        "There are two options in dealing with nulls: \n",
        "\n",
        "1. Get rid of rows or columns with nulls\n",
        "2. Replace nulls with non-null values, a technique known as **imputation**\n",
        "\n",
        "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZY-9QvOKnoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.isnull()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2uOLmZOKno2",
        "colab_type": "text"
      },
      "source": [
        "Notice `isnull()` returns a DataFrame where each cell is either True or False depending on that cell's null status.\n",
        "\n",
        "To count the number of nulls in each column we use an aggregate function for summing (we'll talk more about aggregation, later): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tzZaT_NKno7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rha6b8r5Kno-",
        "colab_type": "text"
      },
      "source": [
        "`.isnull()` just by itself isn't very useful, and is usually used in conjunction with other methods, like `sum()`.\n",
        "\n",
        "We can see now that our data has **128** missing values for `revenue_millions` and **64** missing values for `metascore`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhWp-ZnNxuHm",
        "colab_type": "text"
      },
      "source": [
        "### Removing null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX-UaTFEKno-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Data Scientists and Analysts regularly face the dilemma of dropping or imputing null values. This decision requires intimate knowledge of your data and its context. Overall, removing null data is only suggested if you have a small amount of missing data.\n",
        "\n",
        "Remove nulls is pretty simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0kBkfdpKnpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaN2eNTKnpD",
        "colab_type": "text"
      },
      "source": [
        "This operation will delete any **row** with at least a single null value, but it will return a new DataFrame without altering the original one. You could specify `inplace=True` in this method as well.\n",
        "\n",
        "So in the case of our dataset, this operation would remove 128 rows where `revenue_millions` is null and 64 rows where `metascore` is null. This obviously seems like a waste since there's perfectly good data in the other columns of those dropped rows. That's why we'll look at imputation next.\n",
        "\n",
        "Other than just dropping rows, you can also drop columns with null values by setting `axis=1` in the `dropna()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfB40gKT4ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.dropna(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7TVWX_x7SXx",
        "colab_type": "text"
      },
      "source": [
        "### Imputing null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir1VUt07Vhx",
        "colab_type": "text"
      },
      "source": [
        "Imputing missing values (making educated guesses for them based on values that we do have in our data) is a complicated business and it should not be done without an intimate knowledge of the dataset you're working on. In many cases, and particularly when you don't have intimate knowledge of the data, it may be better to remove missing values than to try to impute them.\n",
        "\n",
        "One simple method for imputing data is to replace it with the average value from the data. This can be done with varying degrees of specificity. For example, we could find the average value for only similar items in the data (e.g., for movies, we might decide to only take the average from the same year from movies that share similar genres).  This is a complicated business and we're not really ready to do this with our current knowledge of pandas. Instead,we can simple impute the values to be the average of all movies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K-QG_798Jp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "revenue_mean = movies_df.revenue_millions.mean()\n",
        "print(revenue_mean)\n",
        "temp_df = movies_df.revenue_millions.fillna(revenue_mean)\n",
        "temp_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubArpkthX9bh",
        "colab_type": "text"
      },
      "source": [
        "# Applying functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ93mgkZKnrJ",
        "colab_type": "text"
      },
      "source": [
        "It is possible to iterate over a DataFrame or Series as you would with a list, but doing so — especially on large datasets — is very slow.\n",
        "\n",
        "An efficient alternative is to `apply()` a function to the dataset. For example, we could use a function to convert movies with an 8.0 or greater to a string value of \"good\" and the rest to \"bad\" and use this transformed values to create a new column.\n",
        "\n",
        "First we would create a function that, when given a rating, determines if it's good or bad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZQvnS8d4yhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zosAOILb4yUa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoUHlGUXKnrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rating_function(x):\n",
        "    if x >= 8.0:\n",
        "        return \"good\"\n",
        "    else:\n",
        "        return \"bad\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o76ABELeKnrS",
        "colab_type": "text"
      },
      "source": [
        "Now we want to send the entire rating column through this function, which is what `apply()` does:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvOASBcbKnrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(rating_function)\n",
        "\n",
        "movies_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yugSNoguKnrV",
        "colab_type": "text"
      },
      "source": [
        "The `.apply()` method passes every value in the `rating` column through the `rating_function` and then returns a new Series. This Series is then assigned to a new column called `rating_category`.\n",
        "\n",
        "You can also use anonymous functions as well. This lambda function achieves the same result as `rating_function`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoQ8MFjsKnrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(lambda x: 'good' if x >= 8.0 else 'bad')\n",
        "\n",
        "movies_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho9QSWbRKnrX",
        "colab_type": "text"
      },
      "source": [
        "Overall, using `apply()` will be much faster than iterating manually over rows because pandas is utilizing vectorization.\n",
        "\n",
        "> Vectorization: a style of computer programming where operations are applied to whole arrays instead of individual elements —[Wikipedia](https://en.wikipedia.org/wiki/Vectorization)\n",
        "\n",
        "A good example of high usage of `apply()` is during natural language processing (NLP) work. You'll need to apply all sorts of text cleaning functions to strings to prepare for machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ocdq0qNYDjS",
        "colab_type": "text"
      },
      "source": [
        "# Brief Plotting with Matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSdiJy9UKnrY",
        "colab_type": "text"
      },
      "source": [
        "Another great thing about pandas is that it integrates with Matplotlib, a popular plotting library, so you get the ability to plot directly off DataFrames and Series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56n2Ks7eKnrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 20, 'figure.figsize': (20, 10)}) # set font and plot size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2HR35vKKnrb",
        "colab_type": "text"
      },
      "source": [
        "Now we can begin. There won't be a lot of coverage on plotting, but it should be enough to explore you're data easily.\n",
        "\n",
        "**Side note:**\n",
        "For categorical variables utilize Bar Charts and Boxplots.  For continuous variables utilize Histograms, Scatterplots, Line graphs, and Boxplots.\n",
        "\n",
        "Let's plot the relationship between ratings and revenue. All we need to do is call `.plot()` on `movies_df` with some info about how to construct the plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xqZhxOUKnrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.plot(kind='scatter', x='rating', y='revenue_millions', title='Revenue (millions) vs Rating');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcsxLD73Knrg",
        "colab_type": "text"
      },
      "source": [
        "What's with the semicolon? It's not a typo, just a way to hide the `<matplotlib.axes._subplots.AxesSubplot at 0x26613b5cc18>` output when plotting in Jupyter notebook environments.\n",
        "\n",
        "If we want to plot a simple Histogram based on a single column, we can call plot on a column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT4V925CKnrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['rating'].plot(kind='hist', title='Rating');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS5dvRPoKnrj",
        "colab_type": "text"
      },
      "source": [
        "Do you remember the `.describe()` example at the beginning of this lecture? Well, there's a graphical representation of the interquartile range, called the Boxplot. Let's recall what `describe()` gives us on the ratings column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2PvRMt3Knrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['rating'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMTLhQgSKnrl",
        "colab_type": "text"
      },
      "source": [
        "Using a Boxplot we can visualize this data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOMzg__OKnrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['rating'].plot(kind=\"box\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CplE-HhgKnrr",
        "colab_type": "text"
      },
      "source": [
        "Here is how to read a Boxplot:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://i1.wp.com/flowingdata.com/wp-content/uploads/2008/02/box-plot-explained.gif\" />\n",
        "    <figcaption>Source: *Flowing Data*</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "By combining categorical and continuous data, we can create a Boxplot of revenue that is grouped by the Rating Category we created above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M7MiqZ0Knrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.boxplot(column='revenue_millions', by='rating_category');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxplPiK5Knrx",
        "colab_type": "text"
      },
      "source": [
        "That's the general idea of plotting with pandas. There's too many plots to mention, so definitely take a look at the `plot()` [docs here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) for more information on what it can do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpDN2-xRoPqO",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Make an Adjusted Rating Column, plot the difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efzKkAOPoZW7",
        "colab_type": "text"
      },
      "source": [
        "In the imdb data that we've been looking at, the rating is just the mean score. But some movies have many more votes than others, and this should lend more \"weight\" to their rating.  \n",
        "\n",
        "A good adjusted scoring rule is:\n",
        "```\n",
        " rating_adjusted = rating - (rating - 5)*2**(-log10(votes+1))\n",
        "```\n",
        "(note: to implement this, we can use `np.log10()` )\n",
        "\n",
        "Make a new column called 'rating_adjusted' to implement this. Then, make a scatter plot of (rating_adjusted - rating) vs  rating.\n",
        "\n",
        "note: it's okay if you want to make a column `rating_delta = rating_adjusted - rating`.\n",
        "\n",
        "Now calculate the `rating_delta` and `rating_adjusted` and make a scatterplot with `rating` on the x-axis and `rating_delta` or `rating_adjusted` on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUgRBeY8GqYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trtHRe-EGdZd",
        "colab_type": "text"
      },
      "source": [
        "## Solution: Don't look at this until you've tried it! (You might have to do this on the final!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MykjKGxkryMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "movies_df.head()\n",
        "\n",
        "# Solution using apply:\n",
        "movies_df['rating_adjusted'] = movies_df.apply(lambda row: row['rating']-(row['rating']-5)*2**(-np.log10(row['votes']+1)),axis=1)\n",
        "\n",
        "# Solution w/o using apply (better):\n",
        "movies_df['rating_adjusted'] = movies_df.rating - (movies_df.rating-5)*2**(-np.log10(movies_df.votes+1))\n",
        "movies_df['rating_delta'] = movies_df.rating_adjusted - movies_df.rating\n",
        "movies_df.plot(kind='scatter',x='rating',y='rating_delta',figsize=(20,10));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrGsjfv40g-x",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Plot of Revenue vs Adjusted Rating for only one Genre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp-No7dQnALk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Using the `movies_df` DataFrame, write a function that will plot the scatterplot of Revenue vs Metascore for only one Genre.\n",
        "\n",
        "Your function should:\n",
        "- have an input argument that is a string of the Genre, e.g., 'Horror'\n",
        "\n",
        "\n",
        "Note that the an entry in the genre column contains a comma-separated list of different genres that a movie belongs to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll-oGp9krW1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.genre[1:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIttaza8rd7X",
        "colab_type": "text"
      },
      "source": [
        "However, we can get all the individual unique genres by using Pandas built in string operations on a series:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae-XRHlGoB74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "allGenresConcatenated = movies_df.genre.str.cat(sep=',') # This will return a string by concatenating all the strings in each row of genre, separating them with a ',' \n",
        "allGenres=np.unique(allGenresConcatenated.split(',')) # This will split the string so that we have a list and then use numpy's unique() to get only the unique elements of the list\n",
        "allGenres"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph9oVKRCBkvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'Horror' in 'Action,Adventure,Fantasy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjt9wEtysE5c",
        "colab_type": "text"
      },
      "source": [
        "You may find the following string method of dataframes useful:\n",
        "- If a dataframe `df` has a string columm, `stringCol`, then the method\n",
        " - `df.stringCol.str.contains(someString)` will return `True` if someString is a substring within a value of stringCol.\n",
        "\n",
        "Your goal is to define a function that will return a plot object. The function should make a scatter plot of `rating_adjusted` on the x-axis and `revenue_millions` on the y-axis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x90ZQ9l0IGjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define your function here\n",
        "def plot_rev_vs_rating_adj(genreName):\n",
        "  plot = # fill in your code here with plot = movies_df[SOMETHING].plot(SOMETHING)\n",
        "  plot.set_title(genreName) \n",
        "  return plot\n",
        "\n",
        "# Run your function for the genre's 'Horror' and 'Action'\n",
        "plot = plot_rev_vs_rating_adj('Horror')\n",
        "plot = plot_rev_vs_rating_adj('Action')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR1xchFRJisF",
        "colab_type": "text"
      },
      "source": [
        "## Solution: Don't look at this until you've tried it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV3NdAxfFDf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df[movies_df.genre.str.contains('Horror')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9c0NaLhszQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_rev_vs_rating_adj(genreName):\n",
        "  plot = movies_df[movies_df.genre.str.contains(genreName)].plot(kind='scatter',x='rating_adjusted',y='revenue_millions')\n",
        "  plot.set_title(genreName)\n",
        "  return plot\n",
        "\n",
        "plot=plot_rev_vs_rating_adj('Horror')\n",
        "plot=plot_rev_vs_rating_adj('Action')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHD7IBw2VD8H",
        "colab_type": "text"
      },
      "source": [
        "It would be nice if we could make a boxplot of the distribution of revenue across all genres in the same plot... we'll come back to this later, when we've learned some more tools to help us do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhowOewtjZW2",
        "colab_type": "text"
      },
      "source": [
        "# Indexing Series and DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK-3DhbiStkE",
        "colab_type": "text"
      },
      "source": [
        "You can access a given column of a DataFrame two ways:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7o9l7TXRbZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.genre # property-style column access"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCvdbonCS1XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['genre'] # dictionary-style column access"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSzIyJ1FQk8V",
        "colab_type": "text"
      },
      "source": [
        "You can slice elements of a DataFrame using its index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYXcUBsnGIFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnqwnXF8Qvwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['Prometheus':'Suicide Squad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzzxeKZpVGDp",
        "colab_type": "text"
      },
      "source": [
        "Notice that we can't get the row of a DataFrame by specifying an index value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07_8JXLEVO0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['Prometheus'] # this will throw a key error, because dictionary-style access of a dataframe is keyed on column names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmGWRT7PVWSA",
        "colab_type": "text"
      },
      "source": [
        "We'll see soon how to access a row of a DataFrame based on the index value using `.loc[]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTlS-j_OTGrC",
        "colab_type": "text"
      },
      "source": [
        "You can also combine these with slicing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs_VVaOtTAax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.genre['Prometheus':'Suicide Squad'] # slice a Series using the DataFrame explicit index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08kOjTdTlhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.genre[1:5] # slice a Series using the implicit row index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NJAgVOhP3Q9",
        "colab_type": "text"
      },
      "source": [
        "### Implicit and Explicit Indexes with `.loc[]` and `iloc[]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wb4ppfijcAz",
        "colab_type": "text"
      },
      "source": [
        "As we just learned, you can set an Index for DataFrames and Series (columns of DataFrames). And you can use the row numbers (instead of the index defined) to access or slice a Series (column) as well.\n",
        "\n",
        "Using the row numbers for access is referred to as *implicit* indexing. While using the DataFrame's defined index is referred to as *explicit* indexing.\n",
        "\n",
        "\n",
        "There is one case where things can get confusing, which is when the explicit index has integer values. \n",
        "\n",
        "Let me show you why with an example: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfCiOWHUTL8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.Series(['a', 'b', 'c','d'], index=[1, 3, 5,7])\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnxNBFDFThu8",
        "colab_type": "text"
      },
      "source": [
        "We can always use a slicing operation (with ``:``) to get a range of the rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NevrmHqTY1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[2:4] # this will return a slice of the 2nd and 3rd rows using the implicit row index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVascMSaT4SY",
        "colab_type": "text"
      },
      "source": [
        "But if we ask for a single row with `[]` it will use our explicit index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtdYjgGBUBve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[3] # explicit index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1LRvj_ZUEEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[4] # this will throw a key error, because our data doesn't have a row index of 4 with our explicit (pandas) index  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph7Yim4zTUCo",
        "colab_type": "text"
      },
      "source": [
        "To avoid this implicit vs explicit indexing confusion, Pandas has two special indexer attributes: \n",
        "- `.loc[]` to access the explicit index (the pandas one that we defined)\n",
        "- `.iloc[]` to access the implicit index (the python-style one, as with lists)\n",
        "\n",
        "This allows us to access elements by implicit or explicit index and to slice by index or explicit indexing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3zDxrDwVJNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.iloc[2:4] # this is the python implicit style, so it will return rows 2 and 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu9nXj9uVL4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.loc[1:5] # this is the explicit pandas style index, so it will return rows with index 1,3,5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myP6M8h1WB1w",
        "colab_type": "text"
      },
      "source": [
        "ASIDE: It's a bit funny that slicing with explicit indexing includes the last element of the range specified (``data.loc[1:5]`` includes 5), while slicing with implicit does not (``data.iloc[2:4]`` doesn't include 4). It is defined this way in order for `iloc[]` to be consistent with the way Python usually indexes collections.\n",
        "\n",
        "IMPORTANT: Its good practice to always use `.loc[]` and `.iloc[]` instead of just `[]` whenever possible, to avoid confusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgOfy_SoUs56",
        "colab_type": "text"
      },
      "source": [
        "Another advantage of the `.loc[]` indexer is that it allows us to select a row (or a slice of a row) from a DataFrame with the index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoKyoQOyUtox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.loc['Prometheus'] # Get one row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBLC5vdSmIzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.loc['Prometheus':'Sing'] # Get a slice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5UlWCjBV6u_",
        "colab_type": "text"
      },
      "source": [
        "So we could then do things like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m65Q4x_5V9Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.loc['Prometheus'].genre"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlk1CKbnWFTR",
        "colab_type": "text"
      },
      "source": [
        "### MultiIndexes \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhJljxL-WIP7",
        "colab_type": "text"
      },
      "source": [
        "There are many occasions where the nature of the data we are analyzing would require rows that relate to more than one index.  For example, consider the case where we want to track the populations metrics of US states across different years. \n",
        "\n",
        "We might have data like this:\n",
        "```\n",
        "                    Population\n",
        "California  2000    33871648\n",
        "            2010    37253956\n",
        "New York    2000    18976457\n",
        "            2010    19378102\n",
        "Texas       2000    20851820\n",
        "            2010    25145561\n",
        "```\n",
        "\n",
        "To create a Series that handles this type of data appropriately, we would use a ``MultiIndex``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7A_ScyXWHWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series=pd.Series([33871648,37253956,18976457,19378102,20851820,25145561],\n",
        "                    index=pd.MultiIndex(\n",
        "                        levels=[['California', 'New York', 'Texas'], [2000, 2010]],\n",
        "                        names=['state','year'],\n",
        "                        codes=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]]) # notice how codes are used to link the same state or same year\n",
        "                    )\n",
        "pop_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjBaQy_UZX3t",
        "colab_type": "text"
      },
      "source": [
        "We could have stored this data using a DataFrame instead of as a Series with a MultiIndex, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcu6FhoaZexV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_df=pd.DataFrame({'2000':[33871648,18976457,20851820], '2010':[37253956,19378102,25145561]},\n",
        "                    index=['California', 'New York', 'Texas'])\n",
        "pop_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nORughAVamDI",
        "colab_type": "text"
      },
      "source": [
        "However, what if we wanted to store more than just one dimension of population metrics in our DataFrame? Then we would need a MultiIndex.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f6yNogLatL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_df = pd.DataFrame({'total': pop_series,\n",
        "                       'under18': [9267089, 9284094,\n",
        "                                   4687374, 4318033,\n",
        "                                   5906301, 6879014]})\n",
        "pop_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knszXyyXa_5U",
        "colab_type": "text"
      },
      "source": [
        "Pandas allows us to \"stack\" and \"unstack\" a MultiIndex using the `.stack()` and `.unstack()` methods (we'll talk more about this later):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdIBRMwRMLpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbHC3TRVbQup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_total_df=pop_df.total.unstack()\n",
        "pop_total_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDCNDlYkMWz7",
        "colab_type": "text"
      },
      "source": [
        "Notice that we've lost the \"total\" title when we did this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNWgoZ-FbVih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_under18_df=pop_df.under18.unstack()\n",
        "pop_under18_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02OtphFMbk7t",
        "colab_type": "text"
      },
      "source": [
        "And of course we can stack it back again, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVD8ZYn4bn41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_under18_df.stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnn-WoNYbtbh",
        "colab_type": "text"
      },
      "source": [
        "though notice that the column name is lost when we do this, so we would have to set it again if we wanted to.  \n",
        "\n",
        "Also notice that `.stack()` returned a series, since we only had one column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0khPbspdXpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(pop_under18_df.stack())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q866cQsKfHkp",
        "colab_type": "text"
      },
      "source": [
        "### Working with MultiIndexed DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6wC54-DfUxy",
        "colab_type": "text"
      },
      "source": [
        "When we have a MultiIndexed Series or DataFrame, we can access an element or row like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btIoRdsRMvxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZhccVajfpy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series.loc['California',2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkEQM3teM3op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii55bekqh_yS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_df.loc['California',2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z-hf7Psf-Ow",
        "colab_type": "text"
      },
      "source": [
        "or even slice it, provide that the MultiIndex is sorted (see the pandas documentation for  [sort_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-O32V5qgATq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series.loc['California':'New York']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBrqAa6zggNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series.loc['California':'New York',2000] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_91wI3ShHEB",
        "colab_type": "text"
      },
      "source": [
        "Other types of indexing also works with MultiIndexes, such as boolean masks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTw47KUrhLBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series.loc[pop_series>22000000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgxl-MQchSxA",
        "colab_type": "text"
      },
      "source": [
        "Or even fancy indexing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j0RaBbKhU-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_series.loc[['California','Texas'],2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6BcgofkhVTb",
        "colab_type": "text"
      },
      "source": [
        "# Merge and Join DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2WH9eCAj0kk",
        "colab_type": "text"
      },
      "source": [
        "One essential feature offered by Pandas is its high-performance, in-memory join and merge operations.\n",
        "If you have ever worked with databases, you should be familiar with this type of data interaction.\n",
        "The main interface for this is the ``pd.merge`` function, and we'll see few examples of how this can work in practice.\n",
        "\n",
        "We'll need to define a convenient function ``display()`` that will help us display multiple dataframes in Google Colab (or Jupyter notebook) side by side, so go ahead and run the below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtvvwAPoWn_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class display(object):\n",
        "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
        "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
        "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
        "    </div>\"\"\"\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
        "                         for a in self.args)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
        "                           for a in self.args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCWhOFg3WvMZ",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1w-M0TB-yTDd3KEF229SSwW3Gi9H-fhKR' width=400 float='center'>\n",
        "\n",
        "Pandas has methods for algebraically manipulating relational data (like the kind of data you typically see in a database).  The fundamental method that we'll look is `merge()`. We'll also look a bit at `join()` (though `join()` is really just a wrapper method to add some convenience -- it ends up calling `merge()`).\n",
        "\n",
        "The `merge()` method is the pandas way to implementing SQL-style JOINs with DataFrames. So you have the usual cardinality of JOINs:\n",
        "- one-to-one\n",
        "- one-to-many\n",
        "- many-to-many\n",
        "\n",
        "And you also have the type of JOINs:\n",
        "- inner\n",
        "- outer\n",
        "- left\n",
        "- right\n",
        "\n",
        "All of these can be implmented in pandas with the method `merge()`. Have a quick look at the [merge documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html). Some things to note:\n",
        "- the `how` keyword specifies the join type: inner (default), outer, left, or right \n",
        "- the `on`, `left_on`, and `right_on` keywords specify what columns to join on. If they are omitted, join will infer by matching column names.\n",
        "- In general, the dataframe returned from a merge will discard the index, unless you do a join based on the index using the keywords `left_index` or `right_index`.\n",
        "\n",
        "Let's see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeaEKoPFZeHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start with some data\n",
        "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
        "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
        "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
        "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
        "display('df1', 'df2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z92dOmklZnFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A one-to-one join\n",
        "df3 = pd.merge(df1, df2)\n",
        "df3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6EDYr2YbwOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A one-to-many join\n",
        "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
        "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
        "display('df3', 'df4', 'pd.merge(df3, df4)')\n",
        "\n",
        "# notice, we didn't tell pandas it was one-to-many -- it inferred it from matching on column name 'group'\n",
        "#  we can see that some entries in group and supervisor will be duplicated in the resulting dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YmuUlQFccCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A many-to-many join\n",
        "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
        "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
        "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
        "                               'spreadsheets', 'organization']})\n",
        "display('df1', 'df5', \"pd.merge(df1, df5)\")\n",
        "\n",
        "# Notice again that we didn't specify it was many-to-many -- pandas inferred it based on column names\n",
        "#  also notice how entries in the employees column are repeated now to show that each employee\n",
        "#  can have many skills and each skill can be had by many employees. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNz6UYHNc-3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can always specify what column should be used to join. If it's the same for both df's we're merging, we can do this with 'on':\n",
        "display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzxDyMU3dMt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sometimes you might have two df's that contain differently named columns that you want to join on. In this case we can use the 'left_on' and 'right_on' keywords:\n",
        "df3a = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
        "                    'salary': [70000, 80000, 120000, 90000]})\n",
        "display('df1', 'df3a', 'pd.merge(df1, df3a, left_on=\"employee\", right_on=\"name\")')\n",
        "\n",
        "# Notice that we end up with both 'name' and 'employee' columns in the result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXmofx_pd_3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  we could use .drop('name', axis=1), if we wanted to drop the name column:\n",
        "pd.merge(df1, df3a, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rb9VizJewUS",
        "colab_type": "text"
      },
      "source": [
        "The above showed how to merge dataframes by joining on the columns. But of course, you can also use the indices of dataframes... and even match an index in one df to a column in the other. \n",
        "\n",
        "Here are some examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwq6kX2Te_7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge based on indices:\n",
        "df1a = df1.set_index('employee')\n",
        "df2a = df2.set_index('employee')\n",
        "display('df1a', 'df2a',\"pd.merge(df1a, df2a, left_index=True, right_index=True)\") \n",
        "# notice: the resulting df has and index."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD8mJKm-fQpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pandas provides the join() method to use indices of both df's you are merging, for convenience. \n",
        "#  So we could accomplish the above like this:\n",
        "display('df1a', 'df2a', 'df1a.join(df2a)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds8fbmPrfk1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can match a column to an index when you join.\n",
        "#  Here we use left_index=True to tell pandas to use the index of the left df and right_on='name' to tell\n",
        "#  pandas to use match it to the 'name' column of the right df.\n",
        "display('df1a', 'df3a', \"pd.merge(df1a, df3a, left_index=True, right_on='name')\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EVoKfb8WJVU",
        "colab_type": "text"
      },
      "source": [
        "# Concatenating DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WixGcP_tMWK0",
        "colab_type": "text"
      },
      "source": [
        "On a related note, its also possible to concatenate to DataFrames together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih4xwFxkMpR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just a function to make generating example dataframes easier\n",
        "def make_df(cols, ind):\n",
        "    \"\"\"Quickly make a DataFrame\"\"\"\n",
        "    data = {c: [str(c) + str(i) for i in ind]\n",
        "            for c in cols}\n",
        "    return pd.DataFrame(data, ind)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvc4VXChUns8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's the most common use case of pd.concat(), when the dataframes have the same columns\n",
        "df1 = make_df('AB', [1, 2])\n",
        "df2 = make_df('AB', [3, 4])\n",
        "display('df1', 'df2', 'pd.concat([df1, df2])')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twVOdOJXVAfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To us pd.concat() when no columns are the same, pass axis=1\n",
        "df3 = make_df('AB', [0, 1])\n",
        "df4 = make_df('CD', [0, 1])\n",
        "display('df3', 'df4', \"pd.concat([df3, df4], axis=1)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9VLvIxHT8X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's how pd.concat() works when the dataframes have some but not all columns in common\n",
        "df5 = make_df('ABC', [1, 2])\n",
        "df6 = make_df('BCD', [3, 4])\n",
        "display('df5', 'df6', 'pd.concat([df5, df6],sort=False)') \n",
        "\n",
        "# notice that Pandas fills in NaN's for entries of non-common columns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow--Rj5v0v7B",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Cars, Cars, Cars ( but no motorcycles :[ )    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA_EqG_z0yY6",
        "colab_type": "text"
      },
      "source": [
        "We'll test our knowledge of merging, and concatenating by working with some datasets on cars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8PtQcpk1E-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars1 = pd.read_csv(\"https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/cars1.csv\")\n",
        "cars2_engine = pd.read_csv(\"https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/cars2_engine.csv\")\n",
        "cars2_perf = pd.read_csv(\"https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/cars2_perf.csv\")\n",
        "cars2_info = pd.read_csv(\"https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/cars2_info.csv\")\n",
        "\n",
        "display('cars1','cars2_engine','cars2_perf','cars2_info')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb291hVG1S46",
        "colab_type": "text"
      },
      "source": [
        "Q1: The first dataset `cars1` has some blank columns. Get rid of them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMsTqQN21eaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8NGGbx1gKL",
        "colab_type": "text"
      },
      "source": [
        "Q2: Look at the number of observations in each of the datasets (cars1, cars2_perf, cars2_engine, cars2_info).  Do any of the datasets contain duplicate data? If so, clean them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RWYGw4H1mG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuQ24cNO1qDF",
        "colab_type": "text"
      },
      "source": [
        "Q3: Combine the data in cars2_engine, cars2_perf, and cars2_info into a single dataframe called cars2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzEp6_KN2ctw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC08UFMcJHgC",
        "colab_type": "text"
      },
      "source": [
        "Q4: Get rid of the unnamed column in cars2 and then combine the data in cars1 and cars2 together, to get a final dataframe named cars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WypNhVHvJLwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4qz9NtLhQfP",
        "colab_type": "text"
      },
      "source": [
        "# Grouping and Aggregating with Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Vd96ZrhU4g",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=18zOyb9GjkKiNW8XqaAnCcm4jZK3XB9Z5' width=1000>\n",
        "\n",
        "Grouping works just like GROUP BY in databases.\n",
        "\n",
        "Groupby operations are really a means to apply split the data according to some columns values, apply aggregation, and then recombine. This is illustrated as:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1uAEdHexYDoM2pcU313tYKAsWmMbnqiNy\" width=700>\n",
        "\n",
        "Pandas has all sorts of aggregation methods:\n",
        "- sum() , min(), max(), count(), mean(), median(), quantile(), var(), std(), apply()\n",
        "\n",
        "You can, of course, apply aggregation methods without grouping (in which case the whole dataframe will be treated as a single group).\n",
        "\n",
        "Let's look at some examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1leUjnsc7MI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
        "                   'data': range(6)}, columns=['key', 'data'])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXvLkY43flaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby('key').sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBe5q7wsrxiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display('df',\"df.groupby('key').cumsum()\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwifUhExgcbk",
        "colab_type": "text"
      },
      "source": [
        "To show more interesting examples, we'll take advantage of a dataset that is included in the Seaborn plotting module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYWMscdAgkif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "planets = sns.load_dataset('planets')\n",
        "planets.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWjFSjFug2qP",
        "colab_type": "text"
      },
      "source": [
        "This is a dataset that describes over a thousand extrasolar planets that were discovered, including when and how they were discovered and various attributes of the planets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HL3qf2shm9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "planets.groupby('method')[['orbital_period','distance']].median() # group by method column, then for the columns orbital_period and distance (fancy indexing) take the median in each group"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPIHeQIzhxQ3",
        "colab_type": "text"
      },
      "source": [
        "This shows that different methods of detecting extrasolar planets are sensitive to different scales of orbital periods and distance.\n",
        "\n",
        "You can also group by multiple columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXW-_vxuTNlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "planets.groupby(['year','method']).distance.mean() # group by year and method, then for each group (distinct year and method combination) take the mean of the distance\n",
        "# note that we used the '.' notation to access the column distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TjfO2CmTKmx",
        "colab_type": "text"
      },
      "source": [
        "Let's look at how to aggregate with multiple aggregation functions from a single groupby operation using the `agg()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PBzR-y9iFyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "planets.groupby('method')['orbital_period'].agg(['count','min', 'median', 'max','std']) # group by method column, then for the column orbital period perform the listed aggregations in each group.\n",
        "# Note that we used the '[]' notation to access the orbital_period column\n",
        "# Note that agg() can take a list of aggregation function names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGeVheDJictx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you want to name the columns, you can use agg like this:\n",
        "planets.groupby('method')['orbital_period'].agg(num='count',min_orbital_period='min', median_orbital_period='median', max_orbital_period='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfTP8Yw2mug5",
        "colab_type": "text"
      },
      "source": [
        "It's also possible to filter data according to the aggregation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg6_H3TEnEZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
        "                   'data1': range(6),\n",
        "                   'data2': np.random.randint(0, 10, 6)},\n",
        "                   columns = ['key', 'data1', 'data2'])\n",
        "\n",
        "# A function to filter where the standard deviation of data2 exceeds 4\n",
        "def filter_func(x):\n",
        "    return x['data2'].std() > 4\n",
        "\n",
        "display('df', \"df.groupby('key').std()\", \"df.groupby('key').filter(filter_func)\")\n",
        "\n",
        "# notice that the std() in the filter is applied to the group, so the filtered\n",
        "#  dataframe will have all groups removed that don't meet the criteria"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkQ2p6oJq2lv",
        "colab_type": "text"
      },
      "source": [
        "Additionally, we can transform data after grouping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PFDgh4q8wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby('key').transform(lambda x: x - x.mean()) # center all columns by subtracting the group mean "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ7BhyvXyiHC",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: International Alcohol Consumption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJNQ-zOSzaLo",
        "colab_type": "text"
      },
      "source": [
        "Here, we'll look at some statistics about alcohol consumption across many different countries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUfkQzCza2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "drinks = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/drinks.csv')\n",
        "drinks.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpFVOsT4zhbI",
        "colab_type": "text"
      },
      "source": [
        "Q1: Which continent drinks more beer on average?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZQKD5lkzsz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code your answer here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbKsirYvzvfP",
        "colab_type": "text"
      },
      "source": [
        "Q2: For each continent, print the statistics for wine consumption:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YXEITZNz3Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code your answer here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFV-5YTIz6HK",
        "colab_type": "text"
      },
      "source": [
        "Q3: What is the mean alcohol consumption per contintent for every continent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7EhIdRg0FPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code your answer here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tTEn-ZG0G0K",
        "colab_type": "text"
      },
      "source": [
        "Q4: Using only one line of code, compute the mean, min and max spirit consumption per continent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHAycdIi0bz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code your answer here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj4dVuyCNeIU",
        "colab_type": "text"
      },
      "source": [
        "# Reshaping Data in Pandas (stop here for next lecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmeDAQsgNicp",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1dCn2ZGnplToZAaEcBOREBxZToM_HTuYs' width=1000>\n",
        "\n",
        "\n",
        "Pandas provides many methods to reshape a dataset, and these end up being super important when working with data. We'll talk about\n",
        "\n",
        "- pivot (and pivot_table)\n",
        "- melt\n",
        "- concat\n",
        "- explode (not shown in the pic above, but very useful)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iunDcSeah8TK",
        "colab_type": "text"
      },
      "source": [
        "Let's grab some stock data from the web so that we can look at something interesting.  We'll use module call `pandas_datareader` to help us grab it.\n",
        "\n",
        "Here's how to do it with a single stock and what the resulting dataframe looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGEf9d6Mcb7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import pandas_datareader.data as web\n",
        "\n",
        "# Start date and End date\n",
        "stDate=datetime.datetime(2020,1,1)\n",
        "enDate=datetime.datetime(2020,1,7)\n",
        "\n",
        "amazon_stock_df=web.DataReader(\"AMZN\",'yahoo',stDate,enDate) # read stock data for amazon stock from the yahoo api into a dataframe\n",
        "display('amazon_df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCVY1oy9iNvi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Notice that this returns a dataframe that is indexed by Date with columns describe the stock on that day (High, Low, Open, Close, Volume, Adj Close).\n",
        "\n",
        "Recall some things about indexes we should keep in mind:\n",
        "- we are free to set the index that we want use `.set_index()`\n",
        "- we can change an index into a column via `.reset_index()`\n",
        "\n",
        "- If we have a MultiIndex:\n",
        " - `.unstack()` will shift the last part of it into a column\n",
        " - `.stack()` will shift a column into the last part of the MultiIndex\n",
        "\n",
        "<br>\n",
        "\n",
        "It's a bit boring to look only at one stock, so let's use our knowledge of `pd.concat()` and list comprehensions to grab the open and close prices only for a bunch of stocks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpxCKxr3eToN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stocks=[\"AMZN\",\"MSFT\",\"NVDA\",\"NTDOY\", \"AAPL\"]\n",
        "stocks_df=pd.concat([ web.DataReader(st,'yahoo',stDate,enDate).assign(Stock=st)[['Stock','Open','Close']] for st in stocks ]) # read this line from the inside out\n",
        "stocks_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm-fL4KylDXy",
        "colab_type": "text"
      },
      "source": [
        "See if you can pick apart the second line of the above code\n",
        "```\n",
        "stocks_df=pd.concat([ web.DataReader(st,'yahoo',stDate,enDate).assign(Stock=st)[['Stock','Open','Close']] for st in stocks ])\n",
        "```\n",
        "\n",
        "What is this doing? Let's look at it from the inside out:\n",
        "- the inner `[... for st in stocks]` is a list comprehension\n",
        "- for each element of the stocks list, we are calling: \n",
        " - `web.DataReader(st,'yahoo',stDate,enDate)` which returns a dataframe\n",
        " - and then `.assign(stock=st)` makes a new column called 'stock' and sets all its value for all rows to be the particular stock (e.g., 'AMZN')\n",
        " - the ``[['stock,'Open','Close']]`` is fancy indexing to only choose the columns stock, Open and Close in that order\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_ZzG_CLmp3L",
        "colab_type": "text"
      },
      "source": [
        "OK, now we're ready to do some **reshaping**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E65VEncJug2E",
        "colab_type": "text"
      },
      "source": [
        "## Melting dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdU07_Ss02nV",
        "colab_type": "text"
      },
      "source": [
        "Our original dataframe is presented in \"wide format\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGbOEqk106ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stocks_1day_df=stocks_df.reset_index()\n",
        "stocks_1day_df=stocks_1day_df[stocks_1day_df.Date==enDate].reset_index().drop(columns=['Date','index'])\n",
        "stocks_1day_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0layV7K808VE",
        "colab_type": "text"
      },
      "source": [
        "We can change this to a \"long format\" with the `melt()` method.  We just need to tell it which column will be the \"identifier\" (i.e., the one we'll leave alone) with the `id_vars` keyword."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq89xHworatH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1d_melted_df=stocks_1day_df.melt(id_vars='Stock')\n",
        "s1d_melted_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE0tEsO9c_5j",
        "colab_type": "text"
      },
      "source": [
        "Notice how the variables Open and Close that were columns now are depicted by the rows of the variable column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEezkB-EdNL7",
        "colab_type": "text"
      },
      "source": [
        "## Pivoting dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eVQulF0dP5e",
        "colab_type": "text"
      },
      "source": [
        "Suppose instead that we had started with a dataframe already in the long format and we wanted to \"pivot\" it into the wide format.  We could do this with the `pivot()` method.\n",
        "\n",
        "We just need to tell if which column should be the index with the `index` keyword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf11h6JDduE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1d_melted_df.pivot(index='Stock',columns='variable')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aawtLYSbeCrd",
        "colab_type": "text"
      },
      "source": [
        "In this way, `pivot()` is like the inverse of `melt()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQhHM6edeSHv",
        "colab_type": "text"
      },
      "source": [
        "## Reshaping MultiIndexed dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0vbbvuieXzT",
        "colab_type": "text"
      },
      "source": [
        "So far we have just been working with a really simply dataframe of stocks that contains only the information from a single day. So let's look at the entire data we collected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvS7dw2Leipt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stocks_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzpHcl-emNW",
        "colab_type": "text"
      },
      "source": [
        "The first thing to notice is that the index is Date, but actually the values in each row relate to both the Date and the Stock.  In other words, the dataframe returned by our collection method didn't really index the data properly. Let's fix this right now by defining a MultiIndex:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWyL3Fzke0Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdf=stocks_df.reset_index().set_index(['Date','Stock'])\n",
        "sdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIAiA6RRfBGG",
        "colab_type": "text"
      },
      "source": [
        "Now we could try to use `pivot()` and `melt()` on this dataframe, but actually these are just special cases of two other methods called `unstack()` and `stack()` (respectively) which are designed to work with the more complex scenario of MultiIndexed data.\n",
        "\n",
        "We can use `unstack()` to put this data into long format.  By default, `unstack()` will always \"unstack\" the last part of the index (here: the Stock column) and leave the first part of the index alone:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynrL4Zkxfa6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdf.unstack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM2ve8whffhA",
        "colab_type": "text"
      },
      "source": [
        "Or we could invert this operation with `stack()` to get back to the wide data format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMFRU5qof7P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdf.unstack().stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCsfP9rPgH5p",
        "colab_type": "text"
      },
      "source": [
        "This is exaclty the same as our original `sdf`, except that it is sorted differently (the default is to sort by the order of the parts of the index (so first by Date and then by Stock).  We could sort differently with `sort_values()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD_IzWNKgfHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdf.unstack().stack().sort_values(['Stock','Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-P5rnFFhA07",
        "colab_type": "text"
      },
      "source": [
        "## Use pivot_table to pivot and aggregate multiple values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRiq3Io-hEu5",
        "colab_type": "text"
      },
      "source": [
        "Sometimes we have a rich dataset that we want to pivot, but if we tried to do so, multiple values would be assigned to a given entry.\n",
        "\n",
        "For example, if we start with our `sdf` dataframe and we want to reduce it to a dataframe that is indexed by stock and just has one value for Open and Close, we would need to tell Pandas how to aggregate the multiple values (e.g., an Open price for each date). \n",
        "\n",
        "We can do this by using the `pivot_table()` method which allows us to specify an aggregation function using the `aggfunc` keyword:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcqkjSkNr98u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sdf.pivot_table(index='Stock',values=['Open','Close'],aggfunc='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrOZchY7h6GL",
        "colab_type": "text"
      },
      "source": [
        "This tells us the Open and Close price averaged over all the dates for each stock. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtrGe_7viPZs",
        "colab_type": "text"
      },
      "source": [
        "## Using explode to handle lists in dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnu_gv94iVoC",
        "colab_type": "text"
      },
      "source": [
        "Sometimes we will encounter a scenario where a column will contain lists within it. \n",
        "\n",
        "Consider the following dataframe that describes a customer and the list of pets that they have: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlTeGJJ5imyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pets_df = pd.DataFrame({'customer':['Xia','John','Venki','Vivian','Abdul'],'pets':[['dog','cat'],['cat','goldfish','turtle'],['hamster','cat','dog'],['dog','cat'],['goldfish','dog']]})\n",
        "pets_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYZpFlYoliIX",
        "colab_type": "text"
      },
      "source": [
        "There is a \"long format\" for this dataframe, but we can't get to it with a `stack()` or `melt()` operation, because the values in the pets columns are lists (of varying length -- some people have more pets than others).\n",
        "\n",
        "There is a very useful method to hande this case called `explode()` (I promise it won't blow up your computer). What `explode()` does is to make the elements of the list into rows by repeating the values of the index and any other non-exploded column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1KPpM_vmI6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pets_df.explode('pets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qatB_IamrQx",
        "colab_type": "text"
      },
      "source": [
        "Now the data is in \"long\" format and the information is preserved.  Notice how the index and customer value repeat in order to list out all the pets of each customer.\n",
        "\n",
        "\n",
        "We can also set the index of `pets_df` to be the customer and then explode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l51f2fgZmqAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pets_df.set_index('customer').explode('pets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSAJCR_NnCh7",
        "colab_type": "text"
      },
      "source": [
        "What about the inverse operation? Is there an unexplode?\n",
        "\n",
        "Well not exactly, but we can easily accomplish this using a `groupby` and `apply` operation.  When combined, `apply` will operate on the column of values for each group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDmVHxCUnTga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exploded_df=pets_df.explode('pets')\n",
        "exploded_df.groupby('customer').pets.apply(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zILLMDTKoXsy",
        "colab_type": "text"
      },
      "source": [
        "The only thing to note here is that the returned object is a Series (i.e., just one column), not a dataframe -- though we could easily convert it into a one-column dataframe by wrapping it with `pd.DataFrame()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-zOhC1ouqdG",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Does the number of planets detected by each method change over the years?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKdue5g00Yly",
        "colab_type": "text"
      },
      "source": [
        "We want to know how the number of planets detected by each method changes over the years.\n",
        "<br>\n",
        "<br>\n",
        "Want we're after is a dataframe where the index is a MultiIndex of (year,method) (where method is e.g., Radial Velocity, Pulsar Timing, etc.) and there is a column (that we'll name 'number') for the count for each year and method.\n",
        "<br>\n",
        "<br>\n",
        "Once the dataframe is structure like this, we can just call `.plot()`.  if the figure is not sized correctly, try adding the keyword argument `figsize=(width,height)` and replace `width` and `height` by integers e.g. `figsize=(20,10)`.\n",
        "<br>\n",
        "<br>\n",
        "Some hints:\n",
        "- you'll need to group by more than one column to get the MultiIndex\n",
        "- the order of the columns in the groupby matters, because we'll need to use `unstack()` to get the detection method's to become column labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPfsGLO0PmUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enter your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMmqhisRPoTG",
        "colab_type": "text"
      },
      "source": [
        "## Solution: Don't look until you've tried it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCpNkubku42x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pmot_df=planets.groupby(['year','method'])['number'].count() # this will produce a multiIndexed df where index is (year, method) and one column: count\n",
        "pmot_df.unstack().plot(); # This will plot a series for each column vs the index (year)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnEyGAVg2z87",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Make a boxplot of movie revenue by each genre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJhoSq7M2KKr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "For this exercise, we'll use `movies_df`, containing imdb movie data.  What we'd like to do is make a boxplot with a box for each genre to show how the revenue is distributed for movies that are part of that genre.\n",
        "<br>\n",
        "<br>\n",
        "Recall how the imdb movies data handles the fact that a movie could belong to one or more genres using a comma-separated string:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7AAUwRmsUDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df[['genre']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sszaW6_vsjih",
        "colab_type": "text"
      },
      "source": [
        "To make our boxplot, we need to get this dataframe into \"long\" format.  If the genre field was a list instead of a string, we would be in a similar situation as with the pets example. So how can we get it to be come a list?  \n",
        "\n",
        "Pandas has some cool methods that let you work with columns that are strings.\n",
        "\n",
        "In fact, many are vectorized versions of Pythons regular string methods:\n",
        "\n",
        "\n",
        "|             |                  |                  |                  |\n",
        "|-------------|------------------|------------------|------------------|\n",
        "|``len()``    | ``lower()``      | ``translate()``  | ``islower()``    | \n",
        "|``ljust()``  | ``upper()``      | ``startswith()`` | ``isupper()``    | \n",
        "|``rjust()``  | ``find()``       | ``endswith()``   | ``isnumeric()``  | \n",
        "|``center()`` | ``rfind()``      | ``isalnum()``    | ``isdecimal()``  | \n",
        "|``zfill()``  | ``index()``      | ``isalpha()``    | ``split()``      | \n",
        "|``strip()``  | ``rindex()``     | ``isdigit()``    | ``rsplit()``     | \n",
        "|``rstrip()`` | ``capitalize()`` | ``isspace()``    | ``partition()``  | \n",
        "|``lstrip()`` |  ``swapcase()``  |  ``istitle()``   | ``rpartition()`` |\n",
        "\n",
        "But they also have methods for pattern matching with regular expressions:\n",
        "\n",
        "| Method | Description |\n",
        "|--------|-------------|\n",
        "| ``match()`` | Call ``re.match()`` on each element, returning a boolean. |\n",
        "| ``extract()`` | Call ``re.match()`` on each element, returning matched groups as strings.|\n",
        "| ``findall()`` | Call ``re.findall()`` on each element |\n",
        "| ``replace()`` | Replace occurrences of pattern with some other string|\n",
        "| ``contains()`` | Call ``re.search()`` on each element, returning a boolean |\n",
        "| ``count()`` | Count occurrences of pattern|\n",
        "| ``split()``   | Equivalent to ``str.split()``, but accepts regexps |\n",
        "| ``rsplit()`` | Equivalent to ``str.rsplit()``, but accepts regexps |\n",
        "\n",
        "I mentions these not only because they are very useful when working with text data, but also because we can use them for this exercise in order to take the genre, which is a string of comma-separated genre names, and convert it into a list.\n",
        "\n",
        "Here's how:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJtJPtz0tXEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df.genre.str.split(',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcBsj7I_tcji",
        "colab_type": "text"
      },
      "source": [
        "We can actually make this a new column of our dataframe. Lets call it \"genre_list\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4xa8lIvtltA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_df['genre_list']=movies_df.genre.str.split(',')\n",
        "movies_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honYOB0ou-Ex",
        "colab_type": "text"
      },
      "source": [
        "Ok, now you're ready to go!  In order to make the boxplot, you'll need to:\n",
        "- use `.explode()` to get the genre_list into long format.\n",
        "- use `.boxplot()` with the appropriate `column` and `by` keyword arguments.\n",
        "- I'd also suggest adding a `figsize=(40,8)` argument to `.boxplot()` because this figure will need to be fairly wide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcHOyY7GAuUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jReiPWlGAzLG",
        "colab_type": "text"
      },
      "source": [
        "## Solution -- Don't look until you've tried it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8kJ4QWQnZ7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SOLUTION: \n",
        "mdf = movies_df.copy()\n",
        "mdf['genreList']=movies_df.genre.apply(lambda x: x.split(',')) # make a genre column that is a list instead of a comma-separated string\n",
        "mdf['genreList']=movies_df.genre.str.split(',') # this does the same thing with a string methbod\n",
        "\n",
        "# Now we can use the 'explode' method which will take a list column and turn it into a bunch of rows\n",
        "mdf[['revenue_millions','rating','genreList']].explode('genreList').boxplot(column='revenue_millions',by='genreList',figsize=(40,8));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5LPCyGRczAR",
        "colab_type": "text"
      },
      "source": [
        "# Transporting dataframes between R (dplyr) and Python (pandas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tEt-MxAdU0V",
        "colab_type": "text"
      },
      "source": [
        "The feather module allows you to save .feather files which can be written and read from either python or R.\n",
        "\n",
        "In python, you can save a dataframe as a feather like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N30H7qByZVOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import feather\n",
        "\n",
        "feather.write_dataframe(movies_df, 'movies.feather')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT5DfcFnd2Fj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "And of course, you can also read a .feather file, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC8D2o40d8Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_movies_df = feather.read_dataframe('movies.feather')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu0YmscieDkP",
        "colab_type": "text"
      },
      "source": [
        "In R, you can read and write dataframes to/from .feather files like this:\n",
        "```\n",
        "library(feather)\n",
        "path <- \"my_data.feather\"\n",
        "write_feather(df, path)\n",
        "df <- read_feather(path)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOKXMJBCU_qF",
        "colab_type": "text"
      },
      "source": [
        "# Reference: Pandas Cheat Sheet\n",
        "\n",
        "Here is a very useful [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) that you will want to reference until you get comfortable working with Pandas.\n"
      ]
    }
  ]
}