{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA865 - Lecture 08.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "jJas2SQWMUcb",
        "Dvw-qVW6Jeb9",
        "sLpZ8sUbJjX0"
      ],
      "authorship_tag": "ABX9TyPZuVW7dDu/mKBtJc5OdMdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/BA865/blob/master/BA865_Lecture_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QAC0qnpIpqW",
        "colab_type": "text"
      },
      "source": [
        "Before we begin, go to Runtime->Change runtime type and make sure GPU is selected -- we'll want to be able to use a GPU for training our neural networks.\n",
        "\n",
        "Note: sometimes using tensors in the GPU can be a pain, because the error reporting is not as nice. For this reason, when developing a NN, it is best to do so using the CPU. Once you have it all sorted out, you can add a little bit of code to change it to the GPU. However, if you don't want to have to reset the runtime and start from scratch, its best to set the runtime type to GPU so that you have the option to move things to the GPU at any point. If you don't then the \"machine\" you are running your code on will not have access to a GPU at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4oWkG_ETgeC",
        "colab_type": "text"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5NVjxzRTgBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "if not os.path.exists('./models'):\n",
        "  os.mkdir('./models')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO7_L8xo7xnl",
        "colab_type": "text"
      },
      "source": [
        "# Defining Neural Network Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7UwfaeSYUr",
        "colab_type": "text"
      },
      "source": [
        "In the past examples, we have discussed multi-layered neural networks, but haven't actually shown any. We built a simply linear model using `nn.linear()`, but how do you build more complex architectures by using pytorch's existing layers?  \n",
        "\n",
        "There are two conventional ways to do this:\n",
        "1. Chain layers together using `torch.nn.Sequential()`\n",
        "2. Create a class for your neural network that inherits from the `torch.nn.Module` base class and  implements the `forward()` method in it.\n",
        "\n",
        "I will show examples of these two approaches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ITc86b7sxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.nn.Sequential(torch.nn.Linear(3,8),torch.nn.ReLU(),torch.nn.Linear(8,2),torch.nn.ReLU(),torch.nn.Softmax(dim=0))\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXdZAq5ifMVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.rand(5,3)\n",
        "model(torch.rand(5,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51mGMBOooALV",
        "colab_type": "text"
      },
      "source": [
        "The sequential approach works when you have a simple network, but an alternative (and much more configurable and robust) approach is to define a class with a constructor and a `forward()` method: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5MLphuNVi7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    # The constructor calls the base class constructor and then defines the layers that will be used (ordering doesn't matter here, as layers are just properties)\n",
        "    super().__init__()\n",
        "    self.fc1 = torch.nn.Linear(3,8)\n",
        "    self.relu1 = torch.nn.ReLU()\n",
        "    self.fc2 = torch.nn.Linear(8,2)\n",
        "    self.relu2 = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=0)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # The forward() method describes how an input tensor (the argument x) will be passed through the layers.\n",
        "    # Here, the order matters.\n",
        "    # Also note that we can do other things to the data at any point between the layers (such as functionally transform it in some way)\n",
        "    #  -- we could add noise to the data somewhere in between some layers, normalize it, randomly drop or forget some of it... etc.\n",
        "    #  Advance NN approaches will often use such tricks. \n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aIIZUsSTdKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BasicNet()\n",
        "y = model.forward(torch.rand(5,3))\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLIsjGq0WDby",
        "colab_type": "text"
      },
      "source": [
        "Note: you can actually define a class and also use `torch.nn.Sequential()` to make logical blocks of layers within your class.  For example:\n",
        "```python\n",
        "class SomeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc_block1 = torch.nn.Sequential(torch.nn.Linear(3,8),torch.nn.ReLU())\n",
        "    self.fc_block2 = torch.nn.Sequential(torch.nn.Linear(8,2),torch.nn.ReLU())\n",
        "    self.softmax = torch.nn.Softmax(dim=0)\n",
        "...\n",
        "```\n",
        "\n",
        "This is convenient if you want to organize your layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoqnQPQEQFSR",
        "colab_type": "text"
      },
      "source": [
        "I'd like to turn our attention to architecture in pytorch but before I do that, because many of the concepts apply to tensor data that is 2D or higher, we'll take a brief foray into `torchvision` and image processing, so I can explain how we represent images as tensors.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJas2SQWMUcb",
        "colab_type": "text"
      },
      "source": [
        "# Working with Image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvw-qVW6Jeb9",
        "colab_type": "text"
      },
      "source": [
        "## MNIST - Handwritten Digit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r-EP0E4Ji3-",
        "colab_type": "text"
      },
      "source": [
        "MINST HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tg0yB94MXXz",
        "colab_type": "text"
      },
      "source": [
        "Pytorch has a bunch of useful utilities for working with image data under the `torchvision` module. \n",
        "\n",
        "Let's look at some example image datasets and how to use some of these utilities in practice. This will put us in a better position to discuss architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wk0QjprUdkU",
        "colab_type": "text"
      },
      "source": [
        "First we'll need to import a bunch of modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2gUetvrP5P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IOHdRAOJ7md",
        "colab_type": "text"
      },
      "source": [
        "We'll use one of the datasets built into torchvision called MNIST, a dataset of 60,000 28x28 grayscale pixel images of handwritten digits, each of which belong to one of ten classes (0-9), that is described in detail [here](http://yann.lecun.com/exdb/mnist/). (btw, **NIST** stands for National Institute of Standards and Technology, who released the first dataset before it was **M**odified by others and thus called **MNIST**). \n",
        "\n",
        "We'll use `torchvision.datasets.MNIST()` but we don't want to work with the raw image data alone as it is delivered as a set of PIL (Python Image Library) image objects. We'll want to convert the single grayscale _channel of the image_ to a tensor and then normalize it so that the values all fall between (-1,1). \n",
        "\n",
        "To accomplish this, we'll use a tool from torchvision's transforms module, `transforms.Compose()` which lets us chain a bunch of transformations together. We'll chain `transforms.ToTensor()` and `transforms.Normalize()`, which takes the mean and std for each of the three color channels. If you work on images, there are tons of useful image transformation in torchvision "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGMk4r1WLOXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chain a bunch of transformations together us torchvision.transforms.Compose\n",
        "transform_mnist = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=(0.5, ), std=(0.5, ))]) # First make the input data a tensor, then apply a normalization to the single grayscale channel of the image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M36yZRA9LeNl",
        "colab_type": "text"
      },
      "source": [
        "With those transforms defined, we can actually grab the dataset and apply the chain of transformations all in one line of code. The method to download and load MNIST also allows you to set the argument `train=True` if you are going to use the images to train a NN (as opposed to testing it). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwg5o5_pLhFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab training data from one of the built-in datasets (MNIST)\n",
        "trainset_mnist = torchvision.datasets.MNIST(root='./mnist', train=True,\n",
        "                                        download=True, transform=transform_mnist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx50-CoxMN0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74mmsm1FMQEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_mnist.data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RwCup4NMSFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_mnist.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf3GOq91MYAo",
        "colab_type": "text"
      },
      "source": [
        "Just to see what one image looks like as a bunch of tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvBGk5FoMYxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleImage, exampleClassLabel = trainset_mnist[0]\n",
        "print(exampleImage.shape) # 1  28x28 tensor -- Each of the 28x28 values represents the intensity of the pixel (white = high intensity ; black = low intensity) \n",
        "print(exampleImage)\n",
        "print(exampleClassLabel) # the class label; we have to look at trainset_mnist.classes to see what this means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLBiRRgxNtlL",
        "colab_type": "text"
      },
      "source": [
        "We'll want some way to show the image, so we'll make a quick function to do this. Because I know we'll work with some color images in a minute, I'm going to define a function that can work with those too.\n",
        "\n",
        "The functions uses `plt.imshow()` which nows how to display a single color channel image if its given as a 2D numpy array (width, height) or a 3 color (R,G,B) image if its given as a 3D numpy array (wdth, height, color).  The function below just does three things:\n",
        " - swaps the dimensions around so they are what `plt.imshow()` expects\n",
        " - undoes our normalization\n",
        " - changes the tensor into a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQuqadEGN6_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "  if img.shape[0]==3: # its probably (color,width,height) so make it (width,height,color) which is what plt.imshow() wants\n",
        "    img = img.permute(1,2,0)\n",
        "  elif img.shape[0]==1: # its probably a (1,width,height) so make it just (width,height) which is what plt.imshow() wants for a single channel\n",
        "    img = img[0]\n",
        "  img = img/2 + 0.5 # undo our normalization, just to show the image, because plt's imshow() expects numbers to be between (0,1)\n",
        "  img = img.numpy() # plt's imshow() knows how to work with numpy arrays, not tensors, so we'll convert it first\n",
        "  plt.imshow(img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U7SCrPAOGAO",
        "colab_type": "text"
      },
      "source": [
        "Now lets grab a random example item from our dataset. We know there are 60,000 ( `trainset_mnist.data.shape[0]` ) items, so we'll use `np.random.randint()` to grab an index in this range, then we'll display just one of the color channels using our custom `imshow()` function. \n",
        "\n",
        "You should run this a few times, to get a feel for the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB_oZML6OWEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exImage,exLabel = trainset_mnist[ np.random.randint(0,trainset_mnist.data.shape[0]) ] # note that exItem is a tuple, so exItem[0] is the image and exItem[1] is the class label index\n",
        "imshow(exImage)\n",
        "print(exLabel) # remember trainset.classes is a list of the class labels, so this will translate the class label index (an int) into the class label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLpZ8sUbJjX0",
        "colab_type": "text"
      },
      "source": [
        "## CIFAR-10: Images of different objects (animals, vehicles, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVamcCiUk3N",
        "colab_type": "text"
      },
      "source": [
        "We'll use one of the datasets built into torchvision called CIFAR10, a dataset of 60,000 32x32 pixel images -- each belonging to one of ten classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) that is described in detail [here](https://www.cs.toronto.edu/~kriz/cifar.html). (btw, CIFAR stands for Canadian Institute for Advanaced Research, though I always think of it as \"Can It Fly And Run\"). \n",
        "\n",
        "Just as we did before for the MNIST example, we'll use `torchvision.datasets.CIFAR10()` but we don't want to work with the raw image data alone as it is delivered as a set of PIL (Python Image Library) image objects. The only difference here is that these images are not a single grayscale channel but are in color. We'll want to convert ***each color channel of the image*** to a tensor and then normalize it so that the values all fall between (-1,1). \n",
        "\n",
        "Again, just as before, we'll `transforms.Compose()` to chain  `transforms.ToTensor()` and then `transforms.Normalize()`. The difference here is that since we have three color channels instead of just one, we supply a 3-tuple for the `mean` and `std` parameters of `Normalize()` to specify the mean and std for each of the three color channels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVskZ4pFQZyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chain a bunch of transformations together us torchvision.transforms.Compose\n",
        "transform_cifar = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]) # First make the input data a tensor, then apply a normalization to each of the 3 color channels of the image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBPmTKU8X0lb",
        "colab_type": "text"
      },
      "source": [
        "Now we'll grab the (training) dataset and apply those transformations in a single line: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw2qJf0AQrxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab training data from one of the built-in datasets (CIFAR10)\n",
        "trainset_cifar = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
        "                                        download=True, transform=transform_cifar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGpdG6hdbAGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_cifar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QtzhuOjcd6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_cifar.data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSwx5LiEbBjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_cifar.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma40agKxYH6K",
        "colab_type": "text"
      },
      "source": [
        "Just to see what one image looks like as a bunch of tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRfrlz2XSmbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleImage, exampleClassLabel = trainset_cifar[0]\n",
        "print(exampleImage.shape) # 3 different 32x32 tensors (one for each color channel) -- Each of the 32x32 values represents the intensity of the color for that pixel \n",
        "print(exampleImage)\n",
        "print(exampleClassLabel) # the class label; we have to look at trainset.classes to see what this means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJtHESSEc4rd",
        "colab_type": "text"
      },
      "source": [
        "Now lets grab a random example item from our dataset. We know there are 50,000 ( `trainset.data.shape[0]` ) items, so we'll use `np.random.randint()` to grab an index int this range, then we'll display the image using our custom `imshow()` function. \n",
        "\n",
        "You should run this a few times, to get a feel for the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiSAmHmfSiS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imshow(exampleImage[0])\n",
        "\n",
        "exImage, exLabel = trainset_cifar[ np.random.randint(0,trainset_cifar.data.shape[0]) ]\n",
        "imshow(exImage)\n",
        "print(trainset_cifar.classes[exLabel]) # remember trainset_cifar.classes is a list of the class labels, so this will translate the class label index (an int) into the class label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RgXChWPPiK3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This same approach can be used to work with other image datasets, though the particulars (the number of pixels, color channels) may differ.\n",
        "\n",
        "Now let's see how we can define and train a NN to classify these images.  We'll start with the MNIST dataset as this is a much simpler classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN0PzKHJX33o",
        "colab_type": "text"
      },
      "source": [
        "# Training a NN to Classify Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpcP121ndjFR",
        "colab_type": "text"
      },
      "source": [
        "Below is a big codeblock!  Let's break it down piece by piece to see what we're doing:\n",
        "\n",
        "1. Import stuff\n",
        "2. Define our `imshow()` function from before\n",
        "3. Define `nnSave()` function to save our NN to a file\n",
        "4. Define `nnLoad()` function to load a NN from a file\n",
        "5. Define a neural net class for use with images\n",
        "- we'll chain together the following layers:\n",
        " - input layer\n",
        " - Fully connected hidden layer 1\n",
        " - ReLU\n",
        " - Fully Connected hidden layer 2\n",
        " - ReLU\n",
        " - Fully Connected hidden layer 3\n",
        " - ReLU\n",
        " - logSoftMax\n",
        "6. A fit function for training our neural network\n",
        "7. A test function for evaluating our neural network\n",
        "\n",
        "Let's have a look at each of these:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpak2tSMR9Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Import Stuff\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# 2. Our imshow() function from earlier\n",
        "def imshow(img):\n",
        "  if img.shape[0]==3: # its probably (color,width,height) so make it (width,height,color) which is what plt.imshow() wants\n",
        "    img = img.permute(1,2,0)\n",
        "  elif img.shape[0]==1: # its probably a (1,width,height) so make it just (width,height) which is what plt.imshow() wants for a single channel\n",
        "    img = img[0]\n",
        "  img = img/2 + 0.5 # undo our normalization, just to show the image, because plt's imshow() expects numbers to be between (0,1)\n",
        "  plt.imshow(img.cpu().numpy()) # plt's imshow() knows how to work with numpy arrays, not tensors, so we'll convert it first\n",
        "\n",
        "# 3. A function to save our NN to a file\n",
        "def nnSave(model,opt,path):\n",
        "  torch.save({'model_class': model.__class__, # this is a pointer to the definition of the model's class\n",
        "              'model_args': model.init_args, # init_args is the only property we have to add to a NN class ourselves for this function to work.\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'opt_class': opt.__class__,\n",
        "              'opt_args': opt.defaults,\n",
        "              'opt_state_dict':opt.state_dict()},\n",
        "            path)\n",
        "\n",
        "# 4. A function to load our NN from a file  \n",
        "def nnLoad(path):\n",
        "  cp = torch.load(path)\n",
        "  model = cp['model_class'](**cp['model_args']) # equivalent to model = ModelClass(arg1,arg2,...)\n",
        "  model.load_state_dict(cp['model_state_dict'])\n",
        "  opt = cp['opt_class'](model.parameters(),**cp['opt_args']) # equivalent to opt = OptClass(arg1,arg2,...)\n",
        "  opt.load_state_dict(cp['opt_state_dict'])\n",
        "  return model, opt\n",
        "\n",
        "\n",
        "# 5. Definition of a Neural Network called ImgNet -- (input layer, FC hidden layer 1, ReLU, FC hidden layer 2, ReLU, FC hidden layer 3, ReLU, logSoftMax)\n",
        "class ImgNet(torch.nn.Module):\n",
        "  def __init__(self,sizeInput,sizeHiddenLayer1,sizeHiddenLayer2,sizeOutput):\n",
        "    self.init_args = {k:v for k,v in locals().items() if k!='self' and k!='__class__'} # this funny line captures the name and values of the args so we can save them w/ nnSave()\n",
        "    super().__init__()\n",
        "    self.fc1 = torch.nn.Linear(sizeInput,sizeHiddenLayer1)\n",
        "    self.relu1 = torch.nn.ReLU()\n",
        "    self.fc2 = torch.nn.Linear(sizeHiddenLayer1,sizeHiddenLayer2)\n",
        "    self.relu2 = torch.nn.ReLU()\n",
        "    self.fc3 = torch.nn.Linear(sizeHiddenLayer2,sizeOutput)\n",
        "    self.logsoftmax = torch.nn.LogSoftmax(dim=1) # We are using dim=1 here because the 0th dimension will be the batch dimension\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.logsoftmax(x)\n",
        "    return x\n",
        "\n",
        "# 6. Fit function for training a NN\n",
        "def fit(num_epochs, model, train_dl, loss_fn, opt):\n",
        "  model.train() # make sure the model is in training mode (instead of eval mode)\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss=0\n",
        "    for xb,yb in train_dl: \n",
        "      xb = xb.view(xb.shape[0],-1) # This will keep the first dimension as the batch dimension and flatten all the others\n",
        "      xb = xb.to(\"cuda\",non_blocking = True) # this puts the tensor in the GPU's memory. non_blocking=True ensures that RAM->GPU RAM copy doesn't block other operations\n",
        "      yb = yb.to(\"cuda\", non_blocking = True)\n",
        "      opt.zero_grad() # We'll start by zero'ing the gradient. We could have done this at the end of this loop, but this ensures we have no errant gradients lying around for the first iteration of the loop\n",
        "      pred = model(xb) # run the input through the model and get the predictions \n",
        "      loss = loss_fn(pred, yb) # calculate the loss -- we'd have to check that the loss_fn gets the prediction and true values in the form it expects -- so its wise to check the docs of whatever loss_fn we use\n",
        "      loss.backward() # propagate the loss backward\n",
        "      opt.step() # tell the optimizer to do its thing\n",
        "      running_loss+=loss.item() # add up the running loss (remember the output of loss will be a scalar, so loss.item() will just be a numerical value)\n",
        "    print(f\"Epoch {epoch} loss = {running_loss/len(train_dl)}\") # print out the loss (averaged over all the predictions in the batch)\n",
        "\n",
        "# 7. Test function for evaluating a NN\n",
        "def test(model, test_dl, loss_fn):\n",
        "  model.eval() # put the model into evaluation mode -- may affect some types of layers (e.g., dropout)\n",
        "  with torch.no_grad():\n",
        "    running_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    numClasses = len(test_dl.dataset.classes)\n",
        "    cm = np.zeros((numClasses,numClasses),dtype=np.int32) # an empty matrix to hold the confusion matrix, we'll sum the confusion matrices for each batch\n",
        "    #print(cm.shape)\n",
        "    for xb, yb in test_dl:\n",
        "      xb = xb.view(xb.shape[0],-1)\n",
        "      xb = xb.to(\"cuda\")\n",
        "      yb = yb.to(\"cuda\")\n",
        "      pred = model(xb)\n",
        "      predLabels = torch.argmax(pred,dim=1)\n",
        "      cm += confusion_matrix(yb.cpu().numpy(),predLabels.cpu().numpy(),range(0,10)) # add this batch's confusion matrix to the total matrix -- we have to specify the list of class indexes, or sklearn will shorten our cm to only the classes seen\n",
        "      loss = loss_fn(pred,yb)\n",
        "      running_loss+=loss.item()\n",
        "    ave_loss = running_loss/len(test_dl)\n",
        "    acc = np.diag(cm)/cm.sum(axis=1) # the per class accuracy is the diagonals (tp) divided by all cases of that class\n",
        "    return cm, acc, ave_loss\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypIGMJn42e6m",
        "colab_type": "text"
      },
      "source": [
        "## Classifying MNIST Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY53SpgBdok8",
        "colab_type": "text"
      },
      "source": [
        "Let's build a neural net to classify MNIST handwritten digit images.\n",
        "\n",
        "The code below repeats what we used earlier to download the mnist dataset and define data loader objects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDUfCiS6swD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the MNIST dataset\n",
        "transform_mnist = transforms.Compose( [transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,)) ] )\n",
        "trainset_mnist = torchvision.datasets.MNIST('./mnist', download=True, train=True, transform=transform_mnist)\n",
        "testset_mnist = torchvision.datasets.MNIST('./mnist', download=True, train=False, transform=transform_mnist)\n",
        "\n",
        "batch_size = 64\n",
        "train_dl_mnist = DataLoader(trainset_mnist, batch_size=batch_size, shuffle=True)\n",
        "test_dl_mnist = DataLoader(testset_mnist, batch_size=batch_size, shuffle=True)\n",
        "imgnet_mnist = ImgNet(28*28,128,64,10).cuda()\n",
        "\n",
        "# Train the NN\n",
        "loss_fn_mnist = torch.nn.functional.nll_loss\n",
        "opt_mnist = torch.optim.SGD(imgnet_mnist.parameters(), lr=0.003, momentum=0.9) # where did I get these \"magic numbers?\"  Trial and error and voodoo.\n",
        "fit(15, imgnet_mnist, train_dl_mnist, loss_fn_mnist, opt_mnist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXyR0-g4eYbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nnSave(imgnet_mnist,opt_mnist,'./models/imgnet_mnist.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TQRjNa8t4y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we had already saved this to a file, we could uncomment the lines below to load it:\n",
        "#imgnet_mist, opt_mnist = nnLoad('./models/imgnet_mnist.pt')\n",
        "#imgnet_mnist = imgnet_mnist.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPLy-Y9AHnkx",
        "colab_type": "text"
      },
      "source": [
        "Before we evaluate the model's performance systematically, let's just get a feel for how it did by looking at some of the images and the model's predictions.\n",
        "\n",
        "The code below will:\n",
        "- grab an item from the data loader (remember it has shuffling, so every time we run it, a different item will be grabbed)\n",
        "- get the NN's predicted label for that image (pay attention to the use of `torch.argmax()`\n",
        "- show the image and show the label\n",
        "\n",
        "We'll run the below a few times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PfN_n-ttKmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab one data item and \n",
        "images, labels = next(iter(test_dl_mnist))\n",
        "img = images[0].to(\"cuda\")\n",
        "label = labels[0].to(\"cuda\").item()\n",
        "\n",
        "with torch.no_grad():\n",
        "  predLabel = torch.argmax(imgnet_mnist(img.view(1,-1))).item()\n",
        "\n",
        "imshow(img.view(28,28))\n",
        "print(f\"Predicted label was: {predLabel} ; Actual label was: {label}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMCR7xpJIT2k",
        "colab_type": "text"
      },
      "source": [
        "Ok, let's turn to formally evaluating the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "446fYlcnXzWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Formally evaluate the models performance on the entire test data using our test function.\n",
        "cm_mnist,acc_mnist,ave_loss_mnist = test(imgnet_mnist, test_dl_mnist, loss_fn_mnist)\n",
        "print(ave_loss_mnist)\n",
        "print(cm_mnist)\n",
        "print(acc_mnist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTFbS6c2ro_J",
        "colab_type": "text"
      },
      "source": [
        "**How did we do?**\n",
        "\n",
        "Our NN classifier performs pretty well. From the confusion matrix, we can see the types of digits that it struggles with (e.g., confusing a \"4\" for a \"9\" or a \"2\" with a \"7\") -- these types of mistakes make sense as you can imagine making the same mistake yourself depending on the person's handwriting. Overall the per-class accuracy is relatively high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxrukg_E27ZJ",
        "colab_type": "text"
      },
      "source": [
        "## Classifying CIFAR10 Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wESQZGIYrXVL",
        "colab_type": "text"
      },
      "source": [
        "Now let's turn to the CIFAR10 dataset and see if a NN with the same architecture can do well in this more complicated context of distinguishing objects from one another. This is a much harder task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQFeLRidszwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the CIFAR10 data\n",
        "transform_cifar = transforms.Compose( [ transforms.ToTensor(),transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) ] )\n",
        "trainset_cifar = torchvision.datasets.CIFAR10(root='./cifar10', train=True, download=True, transform=transform_cifar)\n",
        "testset_cifar = torchvision.datasets.CIFAR10(root='./cifar10', train=False, download=True, transform=transform_cifar)\n",
        "\n",
        "batch_size = 64\n",
        "train_dl_cifar = DataLoader(trainset_cifar, batch_size=batch_size, shuffle=True)\n",
        "test_dl_cifar = DataLoader(testset_cifar, batch_size=batch_size, shuffle=True)\n",
        "imgnet_cifar = ImgNet(3*32*32,128,64,10).cuda()\n",
        "\n",
        "# Train the NN\n",
        "loss_fn_cifar = torch.nn.functional.nll_loss\n",
        "opt_cifar = torch.optim.SGD(imgnet_cifar.parameters(), lr=0.003, momentum=0.9) # where did I get these \"magic numbers?\"  Trial and error and voodoo.\n",
        "fit(15, imgnet_cifar, train_dl_cifar, loss_fn_cifar, opt_cifar)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIbaQjhg3hS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nnSave(imgnet_cifar,opt_cifar,'./models/imgnet_cifar.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AcERS5X3g7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we had already saved this to a file, we could uncomment the lines below to load it:\n",
        "#imgnet_cifar, opt_cifar = nnLoad('./models/imgnet_cifar.pt')\n",
        "#imgnet_cifar = imgnet_cifar.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJKMi40n1w3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print an example\n",
        "images, labels = next(iter(test_dl_cifar))\n",
        "img = images[0].to(\"cuda\")\n",
        "label = labels[0].to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  predLabel = torch.argmax(imgnet_cifar(img.view(1,-1))).item()\n",
        "\n",
        "#imshow(img.permute(1,2,0)) # permute because matplotlib's imshow expects (width,height,color) but we have (color,width,height)\n",
        "imshow(img)\n",
        "print(f\"Predicted label was: {testset_cifar.classes[predLabel]} ; Actual label was: {testset_cifar.classes[label]}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSJTWz863Ngm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Formally evaluate the model's performance on the entire test data using our test function.\n",
        "cm_cifar,acc_cifar,ave_loss_cifar = test(imgnet_cifar, test_dl_cifar, loss_fn_cifar)\n",
        "print(ave_loss_cifar)\n",
        "print(cm_cifar)\n",
        "print(acc_cifar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zmLGp5sXGrC",
        "colab_type": "text"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8lTEc_7WW9H",
        "colab_type": "text"
      },
      "source": [
        "Research into building new types of neural networks has advanced rapidly to solve a rich variety of different machine learning problems in the realms of computer vision, natural language processing, and many other contexts. These advances have lead to all sorts of new types of layers that have been implemented in pytorch.\n",
        "\n",
        "We'll look at the following concepts, the layers associated with them, and discuss the ideas behind them:\n",
        "- Receptive Fields\n",
        "- Pooling\n",
        "- Convolution\n",
        "- BatchNorm\n",
        "- Dropout\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgWQ1GIudMd_",
        "colab_type": "text"
      },
      "source": [
        "## Receptive Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfJ9i1ndMDm",
        "colab_type": "text"
      },
      "source": [
        "We have seen fully connected layers, where each neuron receives an input from all the neurons in the previous layer. However, what if we wanted to construct a layer this is much sparser? What if we wanted each neuron to receive inputs from only some neurons in the previous layer? Consider a 2D grid of neurons as the input layer. A good example of this is the input layer for image data. Now suppose we want each neuron in the next layer to receive inputs from only a small \"window\" (or \"slice\" or \"chunk\") from the input layer. We call this \"window\" the **<font color=blue>receptive field</font>**.\n",
        "\n",
        "---\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>\n",
        "Receptive Fields\n",
        "</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=1DjUfLYPHC5wln9-q4pa2Y-q1hlYsEiXr\" width=450>\n",
        "</center>\n",
        "<figcaption>\n",
        "<font color=669999>The picture above shows an 11x11 2D input layer and a 3x3 receptive field.<br> It also shows a single neuron in the next layer that receives the receptive<br> field as inputs. Notice that this is very different from a fully connected<br> layers (where each neuron in the next layer receives an input from all the<br>neurons in the previous layer). It is much sparser.</font>  \n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "\n",
        "With images, the receptive field is typically a set of adjacent pixels in part of the image. With text applications, this could be numerical values that represent the words surrounding a particular word. It makes sense that we would want to capture some \"local information\" within the data.\n",
        "\n",
        "Because we want often want to define an entire layer that operates on a set of receptive fields, we don't define each receptive field manually, but instead define:\n",
        "- the **receptive field size**, which is the \"window size\" (e.g., 3x3 in the image above)\n",
        "- the **stride**, which is how much in each direction the field should move to define the next receptive field.\n",
        "\n",
        "---\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>\n",
        "Receptive Fields and Stride\n",
        "</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=1wBPzGF4YMRL1zOpVo0dupPRnKdnX1ka3\" width=600>\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>The picture above illustrates an 11x11 input layer, a 3x3 receptive field, and a stride of 1 in<br>the horizontal direction (you can have different strides in each direction).</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "\n",
        "Because we are moving the sliding window around and we may want to be able to pass over the edges of our input, it is typical to incorporate **padding** by adding some extra zero values around the input:\n",
        "\n",
        "---\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>\n",
        "Receptive Fields and Padding\n",
        "</font>\n",
        "</figtitle>\n",
        "<div align=left >\n",
        "<img src=\"https://drive.google.com/uc?id=1kvwBLFFH_eJdPQKs6o3odhgdEnQnIY7u\" width=450>\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>The picture above shows a 9x9 input layer with 1 layer of padding around<br> both the width and height dimension of the input. It also shows each of<br> the neurons in the next layer that receive the different receptive fields.</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "\n",
        "***How many neurons would make up the output layer?*** \n",
        "\n",
        "Its important to know this, because when we chain together layers in a NN, the size of the output of one layer must match the size of the input of the next layer.  The formula for this (which you can also reason through yourself) is:\n",
        "\n",
        "$O_d = \\frac{I_d + 2*P_d - R_d}{S_d} + 1$\n",
        "\n",
        "where:\n",
        "- $I_d$ is the size of the input layer in the $d^{th}$ dimension\n",
        "- $P_d$ is the padding size in the $d^{th}$ dimension\n",
        "- $R_d$ is the receptive field size in the $d^{th}$ dimension\n",
        "- $S_d$ is the stride in the $d^{th}$ dimension\n",
        "\n",
        "So for the last picture I showed above, the calculation would be:\n",
        "\n",
        "$ O_d = \\frac{9 + 2*1 - 3}{1} + 1 = 9$\n",
        "(which is the same for the vertical and horizontal dimensions of the input).\n",
        "\n",
        "Note that from this formula, you can see that operations on receptive fields (like the pooling and convolution operations that we'll talk about in the next section) can effective shrink the size of data passing from input to output, depending on the choice of stride and padding. For this reason, one should take care with applying multiple operations like that in successive layers -- you *could shrink your data down to nothing!* While I've shown everything with 2D inputs, the notions extend to an aribtrary number of dimensions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtUX6eAKX2O9",
        "colab_type": "text"
      },
      "source": [
        "## Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8xJ-ccUQw08",
        "colab_type": "text"
      },
      "source": [
        "Pooling layers enact some aggregation operation on a receptive field. The aggregation could be: average, max, min, etc. Let's look at some examples for max pooling and average pooling: \n",
        "\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>\n",
        "<div align=left>\n",
        "Pooling: Max and Average\n",
        "</div>\n",
        "</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=1cMZ5wWbwatB3YbYHDShlAFXWX5qc-tYO\" width=400>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://drive.google.com/uc?id=1MhVT3O5Z9pi8DROHKx0Ck_V8K9qTtUJT\" width=400>\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>\n",
        "An example of 2x2 pooling with a stride of 2 for max pooling (left) and average pooling (right).\n",
        "</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "Notice that pooling layers don't have any weights. During backpropagation, the gradient is routed backward to all the neurons in the receptive field that contributed to it. So, for example, in max pooling, only the neuron in the receptive field that held the max value would propagate the gradient backwards (similarly in min pooling). In average pooling, all the neurons in the receptive field would propagate the gradient. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWKc9EXJWvE",
        "colab_type": "text"
      },
      "source": [
        "### How to add a pooling layer in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fBONw-QJb_V",
        "colab_type": "text"
      },
      "source": [
        "It's relatively simple to add a pooling layer in Pytorch, by using the `torch.nn.MaxPool2D` (there's a 1d version as well). We simply have to specify:\n",
        "- `kernel_size` - the size of the receptive field you are pooling. It can be an single integer, such as 2, for a 2x2 square pool, or a tuple such as (2,3) for a rectangular 2x3 pool.\n",
        "- `stride` - the stride, which can be a single integer, such as 2, or a tuple such as (2,1) for striding different amounts in the x and y directions.\n",
        "- `padding` - the amount of padding above and below the input before the pooling is applied (we'll talk more about this later). It can be a single integer for equal padding on the x and y dimensions, or a tuple for different padding in the x and y dimensions.\n",
        "- There are other parameters that we'll typically not specify and just use the defaults, but you can read more about it in the pytorch documentation [here](https://pytorch.org/docs/stable/nn.html#maxpool2d)\n",
        "\n",
        "And, of course, there are also `torch.nn.AvgPool2d()` and a few other types of pooling layers that take similar arguments.\n",
        "\n",
        "Here's how we make a MaxPool2d layer with Pytorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ow9649uN-j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "print(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OePGpFCgOOI2",
        "colab_type": "text"
      },
      "source": [
        "There aren't any parameters for the NN to learn for a pooling layer, because it is just pooling the receptive field and performing some aggregate operation. We will have to think about the shape of the output from such an operation (in many cases the pooling will act to shrink the input) -- so you will have to use the formula above to make sure the dimensions of the output of each layer are matched to the dimensions of the input of the next layer in your NN.\n",
        "\n",
        "Here's a simple example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6vd9GqHPqVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "someInput = torch.rand(1,4,4) # e.g., a single 4x4 example input\n",
        "print(someInput)\n",
        "print(p(someInput)) # max pooling applied to the input\n",
        "# According to the formula, we have (same for both x and y dimension) Od = (4 - 2)/2 + 1 = 2 \n",
        "\n",
        "# You can play around with the parameters of the pooling layer and the dimensions of someInput to see how this works"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k9_V-MgxJvH",
        "colab_type": "text"
      },
      "source": [
        "## Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exPJpvN4xLcM",
        "colab_type": "text"
      },
      "source": [
        "Convolution is similar to pooling, but it introduces a weight for each neuron in the receptive field (RF). The weights form a grid with the same dimensions as the receptive field. We call this grid of weights a **<font color=blue>filter</font>** (also called a **kernel**). The output is simply the sum of each RF neuron times the corresponding filter weight. \n",
        "<br><br>\n",
        "\n",
        "The actual calculation for a convolution (with stride=1) is given by:\n",
        "\n",
        "\n",
        "$ C_{m,n} = \\sum_i\\sum_j Filter_{i,j} * Input_{m-i,n-j}$\n",
        "\n",
        "<br>\n",
        "\n",
        "But it's actually much more intuitive than that. A convolution is just sliding the filter box over the input and multiplying each input terms by the filter weight in the corresponding box and then adding up the result.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>A convolution operation</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=11c_7Pcfl-xjVLTReCO1Rds07DcFhZL-6\" width=600 />\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>\n",
        "A convolution operation on a 4x4 input using a 2x2 filter with a stride of 1 in each direction.\n",
        "</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "Try to calculate the values of the output for the above example to ensure that you understand what is happening here. To get a general sense of what convolution does and why it can be such a powerful tool, we'll look at some animations:\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>How Convolution Works (Animated)</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=1aiJ3Dbb7BkmKeFuw7HcEhasZ1GzOdODP\" width=600>\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>\n",
        "A convolution operation on a 6x6 input using a 3x3 filter (also called a kernel). The output of a<br> convolution operation is sometimes called a feature map.\n",
        "</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "Have a look at the filter (kernel) chosen in the example above. It has positive values on the top row, zero values in the middle row and negative values on the bottom row. \n",
        "\n",
        "<font color=blue>**Q**</font>: What kind of *receptive field input* would yield a large value when multiplied by this filter?  \n",
        "\n",
        "<font color=cc6600>**A**</font>: A receptive field that has a horizontal edge. \n",
        "\n",
        "Do you see why?\n",
        "\n",
        "When you see a convolution operation, you should always pay attention to the filter that is used and get a feel for what it might do. Filters can be shaped to pick out \"contrast\" such as edges at different angles or bright spots. They can also be chosen to \"blur\" or \"smooth out\" an image.\n",
        "\n",
        "Let's look at another example.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>Convolution with Stride > 1</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=11lwVxjqIKiD7FYzwBbZyij7fOyObL3p4\" width=600>\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>\n",
        "A convolution operation on a 7x7 input using a 3x3 filter (also called a kernel), with a stride of 2<br> in each direction.\n",
        "</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "If some of this terminology (e.g., filter, kernel) seems familiar to you, it is no accident. In Image processing (e.g., Photoshop) and in audio processing (e.g., Ableton) applying filters through convolution long predates the use of convolution in neural networks. It is a central mathematical operation in ***digital signal processing***.\n",
        "\n",
        "To show you how it can be useful, have a look at these convolutions applied to an image of the facade of a building (from Idstein, Germany) with different filter choices:\n",
        "\n",
        "---\n",
        "<figure>\n",
        "<figtitle>\n",
        "<font size=5 color=6699FF>Convolution for Edge Detection</font>\n",
        "</figtitle>\n",
        "<div align=left>\n",
        "<img src=\"https://drive.google.com/uc?id=1icO8VUDxDKTmoap8_1hKsi_-1ydA4GYx\" width=600>\n",
        "</div>\n",
        "<figcaption>\n",
        "<font color=669999>Using convolution with different filters can detect edges in different orientations. This is why the<br> output of a convolution is sometimes referred to as a \"Feature Map\", because it can pick out<br> different features of the input.</font>\n",
        "</figcaption>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "<br><br>\n",
        "In neural networks, the weights that make up the filters of convolutional layers are **learned** the same way all NN parameters are learned -- through training (i.e., via backpropagation). The bias is typically shared across all the neurons in the convolutional layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uAsXtvoDqnW",
        "colab_type": "text"
      },
      "source": [
        "### How to add a convolutional layer in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8u-70RKDxMF",
        "colab_type": "text"
      },
      "source": [
        "It's relatively simple to add a convolutional layer into a NN in Pytorch using a `torch.nn.Conv2d()` layer. We simply have to specify:\n",
        "- `in_channels` -- The number of 2d planes or input channels (this is analogous to e.g., the number of color channels of an image -- so with CIFAR10, it would be 3)\n",
        "- `out_channels` -- the number of output channels produced by the convolution. This is the number of filters or kernels that the NN will learn. There will be a filter for each input channel.\n",
        "- `stride` -- the stride parameter. It can be a single number, such as 3, or a tuple such as (1,2) if we want the stride to be different in the x and y dimensions of the input.\n",
        "- `padding` -- the padding around the input. It can be a single number, such as 1, or a tuple such as (1,2) if we want different padding around x and y dimensions of the input.\n",
        "- There are some other parameters that we'll typically just set as defaults. You can read about them in the pytorch documentation [here](https://pytorch.org/docs/stable/nn.html#conv2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP8EWzdBHlmy",
        "colab_type": "text"
      },
      "source": [
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62KGeyuUHnIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "c = torch.nn.Conv2d(in_channels=1,out_channels=3, stride = 1, padding=1, kernel_size=3)\n",
        "print(c.weight.shape)\n",
        "print(c.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYeSr7TdH59C",
        "colab_type": "text"
      },
      "source": [
        "The tensors that are shown above are the filters (i.e., the parameters that our NN will learn when we train it), intialized to some random values. Notice that we specified a single input channel, and 3 output channels, so we have 3 different filters (or kernels), each of shape 3x3.\n",
        "\n",
        "If we wanted, we could make the filter (or kernel) be non-square:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68W3fTIIbX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.nn.Conv2d(in_channels=1,out_channels=3, stride = 1, padding=1,kernel_size=(3,4))\n",
        "print(c.weight.shape)\n",
        "print(c.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG8pcPTCIgg8",
        "colab_type": "text"
      },
      "source": [
        "Notice that this has 3 rectangular filters of shape 3x4 that will be convolved over the single input channel (e.g., a grayscale image).\n",
        "\n",
        "If we have 3 color channels for our image, as is the case with CIFAR10 images, we would specify `in_channels=3` and"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LqcSafsI0VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.nn.Conv2d(in_channels=3,out_channels=3, stride = 1, padding=1,kernel_size=3)\n",
        "print(c.weight.shape)\n",
        "print(c.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH8K89rpJF92",
        "colab_type": "text"
      },
      "source": [
        "Notice that, in this case, we have 3 3x3 filters for each color channel that our NN will be learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpDSgTRGGBSE",
        "colab_type": "text"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAtTHobUGJ0L",
        "colab_type": "text"
      },
      "source": [
        "Batch normalization is a way of keeping the outputs from a layer in your NN from getting out of control. It normalizes the outputs from a layer for each \"batch\" so that the mean activation value is close to 0 and its standard deviation is close to 1. This can lead to substantial speedup in training a NN. These layers are typically used after convolutions as well as fully connected layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ7ZPqjxTiNp",
        "colab_type": "text"
      },
      "source": [
        "### How to add a batch normalization layer in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc_XLVnsTm4Z",
        "colab_type": "text"
      },
      "source": [
        "It's pretty straightforward -- we just use `torch.nn.BatchNorm2d` (for 2d inputs) or `torch.nnBatchNorm1d` (for 1d inputs).\n",
        "\n",
        "We just have to specify `num_features` which is analogous to the number of channels in an image for the 2d case -- i.e., usually we are passing a batch of size N and the image has some Height (H) and Width (w), our tensors going into this will have shape (N,C,H,W). Of course, if you did some convolution in the prior step, then C would have to match the number of out_channels from that convolution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jUUxJKxUpXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bn = torch.nn.BatchNorm2d(1)\n",
        "print(bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9msZskiGLQk",
        "colab_type": "text"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd8fn887GMuM",
        "colab_type": "text"
      },
      "source": [
        "Dropout is a technique for training Neural Networks that tries to make sure that no neuron in the network relies too on other neurons and \"learns\" something.  It is a type of **regularization**. \n",
        "- Regularization is a category of techniques to reduce fitting error and reduce the potential for overfitting). \n",
        "\n",
        "The idea of dropout is that, during ***training time***, we will ignore some neurons with random probability (i.e., pretend as if they are not present in the network): \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1nThNWfnnC5omfMEJIm0JOSocYh6XcJaY\" width=600>\n",
        "\n",
        "By doing so, we encourage neurons to not become entirely dependent on other neurons (which may \"dropout\" with some probability in each training iteration):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1VBsbcHlFcZiwe9SV9eHIH8Icj-gyNfmg\" width=300>\n",
        "\n",
        "Note that **at test time** all neurons will be kept in the network. To ensure that this is the case, we have to indicate to our model when we are training and when we are testing. We do this with `model.train()` and `model.eval()`. You may have notice these lines of code in the function that I supplied above for `fit()` and `test()`. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLbovxqkU6ps",
        "colab_type": "text"
      },
      "source": [
        "### How to add a dropout in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXRHJV9iU5iq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In practice, it is relatively straightforward to add dropout to a neural net in PyTorch using `torch.nn.Dropout()` or `torch.nn.Dropout2d()`. We just need to add a dropout layer after any layer where we might want some of the neurons to be dropped. When we do so, we have to specify the  probability, `p`, that each neuron in the previous layer will be dropped.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_c44TVxVE3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do = torch.nn.Dropout2d(p=0.5)\n",
        "print(do)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBooL456g-EH",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Train a CNN for labeling CIFAR-10 images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1lr7i4hCKQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}