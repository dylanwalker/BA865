{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA865 - Lecture 07.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PZqH0luro3Tv",
        "3eL2GnygpB6Y",
        "917zUBzHi24w"
      ],
      "authorship_tag": "ABX9TyNZaX6PEafTgMOpQKVoA4qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/BA865/blob/master/BA865_Lecture_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZqH0luro3Tv",
        "colab_type": "text"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEtG0MVbovdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49gdNNhso9_S",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eL2GnygpB6Y",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1OFx-0HlKzV2kOVaD2SNkZwOUpHhClMiJ)\n",
        "\n",
        "## What is PyTorch?\n",
        "\n",
        "Pytorch is an open source machine learning framework that we can use to build and train artificial neural networks.\n",
        "\n",
        "\n",
        "## Why PyTorch?\n",
        "\n",
        "You might have heard about a very popular Neural Network library called Tensorflow. And you might be wondering \"Why aren't we learning Tensorflow?\".\n",
        "\n",
        "There are a few answers to this question:\n",
        "- Declarative vs Imperative:\n",
        " - Tensorflow was designed to be **declarative**. In Tensorflow, you set up your neural network architecture as a static graph before a model can run. The graph is full of all of these placeholders that will be replaced with tensors built from data when the model is run.  In this sense, the model is kind of an enclosed box that you define ahead of time, with only a few ways that you can communicate or pass data into this box. \n",
        " - In contrast, PyTorch is **imperative**. You can define, change and pass data through nodes of the graph as you go. This means your ability to change things on the fly and peer into what is happening, and even **debug** easily.\n",
        "- Static vs Dynamic:\n",
        " - Because you build static graphs in Tensorflow, it is harder to implement dynamic neural network architectures. Other dynamical things, such as input data that has varying size have to be handled with workarounds (such as padding the data).\n",
        " - But PyTorch is naturally dynamics and so its relatively easy to do these things.\n",
        "- Pedagogical reasons:\n",
        " - The Tensorflow API is a bit cluttered. There are many ways to do things and it isn't always clear what is the best way to do it. While there is a ton of support online (because the community of Tensorflow users is quite large), it has evolved significantly over the years and you may easily find outdated methods and approaches.\n",
        " - PyTorch is very easy to learn and is more \"Pythonic\".\n",
        "\n",
        "\n",
        "To be clear, there are lots of other differences and Tensorflow has some many advantages. Not to mention that Tensorflow has a new Eager execution framework that allows you to do more dynamic things with it. Its also very easy to deploy in production and can be very computationally efficient. Ultimately if you continue to learn and work with neural networks, you will likely have to learn both frameworks. However, I chose PyTorch as a starting framework because I believe it provides a smoother learning curve.  And once you know PyTorch and the fundamentals of neural networks, it will then be easier to learn Tensorflow if you choose.\n",
        "\n",
        "\n",
        "## What does PyTorch do for us?\n",
        "\n",
        "One of the many thing that pytorch provides is the ability to make and work with tensor objects.\n",
        "\n",
        "A tensor is a number, vector, matrix, or any n-dimensional array.  You might be thinking \"Hey, wait a minute, we already have a library for working with numbers, vectors, matrices and n-dimensional arrays -- its called numpy!\".\n",
        "\n",
        "\n",
        "That's true, but pytorch is different because the tensor objects are built to automatically compute gradients (derivatives) when they are linked to one another through an expression. Computing gradients is an important aspect of the backward propagation step that is used in the training loop to train a neural network.\n",
        "\n",
        "We will see how all of this works together, but for now let's just explore how to create and work with some very basic tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQDb-eIYudT7",
        "colab_type": "text"
      },
      "source": [
        "# Basics of Pytorch tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbqfGqMhKrYk",
        "colab_type": "text"
      },
      "source": [
        "The first thing we need to do is to import the pytorch module and decide whether we want to execute our code on a CPU (Central Processing Unit) or a GPU (Graphics Processing Unit).\n",
        "\n",
        "The operations involved in working with tensors and training neural network can benefit from parellelization significantly. Just as tools such as numpy take advantage of linear algebra libraries to vectorize operations, pytorch can also take advatage of libraries to parallelize computations. Whereas modern CPUs have increased the number of cores over the years, they still pale in comparison to the number of cores on GPUs which can number in the hundreds to thousands. Neural networking frameworks such as Pytorch can leverage libraries, such as CUDA (a parallelel computing framework developed by NVIDIA) to take advantage of all the GPU cores.\n",
        "\n",
        "Of course, in order to do this, we need to be executing the code on a machine that has a GPU.\n",
        "\n",
        "Google Colab offers GPUs and TPUs (Tensor Processing Units) as options under the runtime settings: \n",
        "`Runtime->Change Runtime Type->Hardware Accelerator-> GPU or TPU`. \n",
        "\n",
        "Let's do this now, so that we have access to a GPU.\n",
        "\n",
        "We won't need to do this for now, as we'll be executing relatively shallow neural nets that can still be trained in a reasonable amount of time with just a CPU.\n",
        "\n",
        "\n",
        "When you work with pytorch, you can specify whether to use the CPU or GPU (if one is available). So I will show you code to do this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Cl09mYVmG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available() # This will return True if we have  setup a GPU on this Colab runtime or False otherwise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTE9Y7aV3XS",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how to make some simple tensors. It is very similar to how you create multi-dimensional arrays in numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VatKhQn_V6_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor([[0.,3.0,-3.4],[-2.4,8.0,5.9]]) # A 2x2 tensor of floats\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00wdD4PzWOBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUo3vjE6WdlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOfNUjByWjWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.zeros(2,2) # a 2x2 tensor with all values set to 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrfbf8kDWtTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.rand(1) # a 1-D tensor (scalar) filled with random values."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pInScZXc36",
        "colab_type": "text"
      },
      "source": [
        "You can do all the things with tensors that you can do with numpy, such as the usual arithmetic operations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bszszMvZW2n4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=torch.ones(3,5)\n",
        "y=torch.rand(3,5)\n",
        "print(x)\n",
        "print(y)\n",
        "print(x+y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n05nEqJfXu1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y/x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppargG41X5KX",
        "colab_type": "text"
      },
      "source": [
        "And tensors in torch have their own version of Ufuncs and support broadcasting (just like numpy):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PvQMOqDX_eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y+1 # will add 1 to every element of y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_DUq-6OYObR",
        "colab_type": "text"
      },
      "source": [
        "Tensors can live in the main memory of the machine (if we are going to work with them on the CPU) or they can live in the graphics cards dedicated memory (if we are going to work with them on the GPU):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de6Zyp0oYYV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_cpu_tensor=torch.rand(5,4)\n",
        "some_cpu_tensor.device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrmj44eNYgXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_gpu_tensor=torch.cuda.FloatTensor(5,4).uniform_() # notice the call to do this on the GPU is a bit different\n",
        "some_gpu_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZZI1zBZt7N",
        "colab_type": "text"
      },
      "source": [
        "Notice the property `device` indicates `cuda` (i.e., that it lives on the GPU)\n",
        "\n",
        "We can also move a tensor to the GPU or CPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkdJHdT-Zx5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_cpu_tensor.to(\"cuda\") # This will return a tensor that is identical to the one we created earlier, but that lives in the memory on the GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0vnL9mLZ8Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_gpu_tensor.to(\"cpu\") # This will return a tensor that is identical to the one we created earlier, but the lives in the memory on the CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJqhnpsaj2p",
        "colab_type": "text"
      },
      "source": [
        "Note that the above calls return copies of the tensor that live in the memory of either the CPU or GPU, but they do not affect the original tensor we created. If we want to work with that copy we have to assign it to a variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnjfMg22agRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_cpu_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2gLV568aw_F",
        "colab_type": "text"
      },
      "source": [
        "`some_cpu_tensor` still lives in the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg2CblYga4dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "another_gpu_tensor = some_cpu_tensor.to(\"cuda\")\n",
        "another_gpu_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnRPmODVbE7x",
        "colab_type": "text"
      },
      "source": [
        "Pytorch tensors support all the usual numpy functions. So you should feel right at home working with them. For that reason, I won't go through all the basic things that you can do with tensors. \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxQGhmO2bKwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_gpu_tensor.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pJ-0PLZbeqM",
        "colab_type": "text"
      },
      "source": [
        "# Gradients and Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6qiwJ3FblST",
        "colab_type": "text"
      },
      "source": [
        "At this point, you might again be wondering why we need pytorch if numpy can do all these things already. But there is one very important thing that pytorch tensors do that is very useful for working with neural networks. They can automatically differentiate -- or in other words, they can calculate gradients (multi-dimensional derivatives) when we combine tensors together with mathematical expressions.  This is exactly what is needed for the \"back propagation\" step of training a neural net.\n",
        "\n",
        "When we create a tensor, we can tell pytorch to enable calculation of  gradients for that tensor by specifying the keyword argument `requires_grad=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjDMrlcLc3Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=torch.rand(1,requires_grad=True)\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zZkwwxqg5Uj",
        "colab_type": "text"
      },
      "source": [
        "`requires_grad` is contagious -- any expression that depends on a tensor that requires a gradient will also require a gradient:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNcDzWnAdJLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=x**2 # remember ** means \"raise to the power\"\n",
        "print(y)\n",
        "print(y.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZEslSz4hDj4",
        "colab_type": "text"
      },
      "source": [
        "Two things to notice:\n",
        "1. `y` has a function associated with it called `grad_fn`. The name of the functions gives a hint at what it is for `PowBackward0`. The `Pow` indicates that it is from a power operation and `Backward` indicates that it is used to propagate the gradient backward. \n",
        "2. Because `y` depends on `x` which requires a gradient, it zlso requires a gradient (contagious)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Da3Wd7vhRN0",
        "colab_type": "text"
      },
      "source": [
        "Whenever we have a **scalar tensor** (a tensor that holds only one value, not a vector or multi-dimensional array) that depends on other tensors, we can tell pytorch to calculate the gradient. We do this by calling `y.backward()`. The term backward here refers to back propagation and it is the fundamental way that we train a neural network. By \"train\" I mean, adjust the weights and biases of a NN to minimize the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3L04U_7iMoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.backward() # tell pytorch to calculate the gradients involved in the definition of y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4jXj4hWdS3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'dy/dx = {x.grad}')\n",
        "print(2*x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBbqvULTikDe",
        "colab_type": "text"
      },
      "source": [
        "Notice that pytorch correctly calculated the gradient:  $dy/dx=2x$\n",
        "\n",
        "How this works is that tensorflow takes expressions between tensors and uses them to build a *computational graph* behind the scenes. All operators in this graph are implemented by the **autograd** package. Pytorch can pass data forward through this graph (i.e., start with the input, perform the operations to get the output) and also propagate things backward through this graph by applying the chain rule (remember your calculus?). Autograd is designed in a modular way so that the functions it implements only have to worry about their own role (their own differentiation with respect to inputs and outputs) in the chain rule process.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917zUBzHi24w",
        "colab_type": "text"
      },
      "source": [
        "Some important things to note here:\n",
        "- Automatic differentiation is all handled for us in the background by autograd -- we won't actually need to manually look at `.grad`s\n",
        "- By default, the `.grad` values accumulate. This is useful for pytorch do \"do its thing\" when we tell it to backward propagate the loss. But if we wanted to handle some portions of this process manually, we have to remember that gradients will accumulate unless we set them to zero. \n",
        " - We can zero gradients with a call to `INSERT HERE`\n",
        " - we'll actually do this today when we manually implement linear regression.\n",
        "\n",
        "\n",
        " ## How does autograd do it's thing?\n",
        " * At this stage, you don't need to know... and you probably don't want to know.\n",
        " - Ok, really want to know? Have a [look at this video](https://youtu.be/MswxJw-8PvE) or [this blog post](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/).\n",
        " - Autograd is a really clever implementation of automatic differentiation. So, naturally it is not trivial to understand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XwgehineanS",
        "colab_type": "text"
      },
      "source": [
        "# Getting a scalar or a numpy array from a tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMmlgIjael7H",
        "colab_type": "text"
      },
      "source": [
        "There are some circumstances where you might want to get the value of a scalar rather than as a tensor type.  You can do this with the `.item()` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kx_TfI7exdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "st = torch.tensor([5.2])\n",
        "st.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWCSB21GfDEF",
        "colab_type": "text"
      },
      "source": [
        "In some circumstances, we might want to get a tensor as a numpy array.  If we want to do this, we must:\n",
        "-  use the `.detach()` method to get rid of any gradient part of the tensor and keep only the array part\n",
        "- then we can use the `.numpy()` method to get the detached tensor as a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQf5qzTfgfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "at = torch.rand(10,requires_grad=True)\n",
        "at.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFs0Rp8yKsMh",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression with Pytorch -- the manual way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_CMNQsIugOj",
        "colab_type": "text"
      },
      "source": [
        "Now we will illustrate how pytorch can be used to implement the main training loop in training a neural network: \n",
        "- forward computation to get the result and calculate the loss by comparing the result to the \"right answer\"\n",
        "- backward propagation to adjust the weights to minimize the loss\n",
        "\n",
        "We'll do this through a simple example of linear regression.\n",
        "\n",
        "The data that we'll work with is data on Apple and Orange crop yields from different geographic regions and average data on temperature, rainfall and humidity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRz42bf1AP2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "crop_file='https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/crop_yield.csv'\n",
        "crop_df=pd.read_csv(crop_file)\n",
        "crop_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_34yutACwH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs=np.array(crop_df.iloc[:,1:4],dtype='float32')\n",
        "inputs=torch.from_numpy(inputs)\n",
        "inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXrhlRtUErqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "targets=np.array(crop_df.iloc[:,4:],dtype='float32')\n",
        "targets=torch.from_numpy(targets)\n",
        "targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKa92eiIFTOs",
        "colab_type": "text"
      },
      "source": [
        "We want to make a linear \"neural network\" that relates the inputs to the targets. In other words we want to implement the equation:\n",
        "\n",
        "\n",
        "$$\n",
        "\\hspace{1cm} Y\\hspace{1cm}=\\hspace{1.cm}X \\hspace{2.1cm} \\times \\hspace{1.cm} W^T \\hspace{1.cm}  + \\hspace{1cm} b \\hspace{1cm}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\left[ \\begin{array}{cc}\n",
        "y_{11} & y_{21} \\\\\n",
        "y_{12} & y_{22} \\\\\n",
        "y_{13} & y_{23} \\\\\n",
        "\\end{array} \\right]\n",
        "%\n",
        "=\n",
        "%\n",
        "\\left[ \\begin{array}{cc}\n",
        "73 & 67 & 43 \\\\\n",
        "91 & 88 & 64 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "69 & 96 & 70\n",
        "\\end{array} \\right]\n",
        "%\n",
        "\\times\n",
        "%\n",
        "\\left[ \\begin{array}{cc}\n",
        "w_{11} & w_{21} \\\\\n",
        "w_{12} & w_{22} \\\\\n",
        "w_{13} & w_{23}\n",
        "\\end{array} \\right]\n",
        "%\n",
        "+\n",
        "%\n",
        "\\left[ \\begin{array}{cc}\n",
        "b_{1} & b_{2} \\\\\n",
        "b_{1} & b_{2} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "b_{1} & b_{2} \\\\\n",
        "\\end{array} \\right]\n",
        "$$\n",
        "\n",
        "We'll start with some random weights and biases -- this is typically how we \"initialize\" the parameters of a neural network -- set them to some random values to start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrHZgWuIBGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReuGkRewIvjl",
        "colab_type": "text"
      },
      "source": [
        "We'll define our model according to the above equation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGSkN6-LFSi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model\n",
        "def model(x):\n",
        "    return x @ w.t() + b  #note that @ is the matrix multiplication operator in numpy or pytorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E_mX_qkJKPI",
        "colab_type": "text"
      },
      "source": [
        "We took advantage of broadcasting in the above (since we only have two biases). We also took the transpose of the weight matrix with `.t()` (so that the dimensions match to permit a matrix multiplication). \n",
        "\n",
        "Our model is ready to generate predictions -- although, they won't be very good ones yet because we haven't trained it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MW3JX1HIr5U",
        "colab_type": "code",
        "outputId": "44581e19-deb9-42df-cd03-e95d62652a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[147.1365, -27.5695],\n",
            "        [199.1310, -52.1653],\n",
            "        [208.1984, -77.7347],\n",
            "        [154.2487,  27.4991],\n",
            "        [191.0965, -89.3801]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFTn4XsVJp8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare with targets\n",
        "print(targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gf19qVaJxC-",
        "colab_type": "text"
      },
      "source": [
        "The next step is to define a loss function, that describes how (poorly) our predictions match the targets. We'll use the mean squared error, since we are implementing linear regression. Different circumstances would require a potentially different loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTiBBjxiKEZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MSE loss\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo-8lLvdKMoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E_EvUXGKihT",
        "colab_type": "text"
      },
      "source": [
        "We can now calculate the gradients of the loss with respect to the weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2vNLsITKbOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9dwjKznKdhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradients for weights\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J53qPdf6Kg5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradients for bias\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfLohkMK8Rp",
        "colab_type": "text"
      },
      "source": [
        "We know about derivatives and optimization from calculus:\n",
        "![](https://drive.google.com/uc?id=1ywzITKPARYGqM8eS_ha7OFbD4g0wF7fg)\n",
        "\n",
        "if the gradient is positive: \n",
        "- increasing the variable will increase the loss \n",
        "- decreasing the variable will decrease the loss \n",
        "\n",
        "If the gradient is negative:\n",
        "- increasing the variable will decrease the loss \n",
        "- decreasing the variable will increase the loss \n",
        "\n",
        "\n",
        "For now, we'll zero the gradients that for our `w` and `b` tensors, as we'll want to start the training process with no gradients accumulated:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVsvwqIqOw25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhvTMBU5OlM5",
        "colab_type": "text"
      },
      "source": [
        "We'll use gradient descent as the rule for adjusting our weights and biases:\n",
        "```\n",
        "w -= w.grad * 1e-5\n",
        "b -= b.grad * 1e-5\n",
        "```\n",
        "This is in accordance with the intuition we developed above. We chose to multiply the gradient by a small number (which adjusts how big of a step we take). This is called the \"learning rate\".\n",
        "\n",
        "Now we are ready to implement our training loop. We will pass over our data several times to keep adjusting the weights and biases.  Each pass is called an *epoch*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzEPmrksQfo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train for 100 epochs\n",
        "losses=[]\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    preds = model(inputs)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    losses.append(loss.item())\n",
        "    with torch.no_grad(): # this ensures that gradients won't be affected in the codeblock -- we already have the values of the gradient from the loss.backward() call\n",
        "        w -= w.grad * 1e-5 \n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_() # zero the weight gradient after we adjusted the weight\n",
        "        b.grad.zero_() # zero the bias gradient after we adjusted the bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOO9dXVRBL1",
        "colab_type": "text"
      },
      "source": [
        "Ok, let's see how we did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF7Q0TMzR0Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses);\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOCeTpgPRFrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4QgGaqRIgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot predictions vs targets\n",
        "#  In order to get from tensors to numpy arrays that we can plot, we'll have to:\n",
        "#     - call detach() to return a copy with just the data and not the gradient\n",
        "#     - call numpy() to return a numpy array, since matplotlib doesn't know how to plot tensors\n",
        "plt.plot(targets.detach().numpy(),preds.detach().numpy(),'.');\n",
        "plt.xlabel('targets');\n",
        "plt.ylabel('predictions');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi01cs9zRXBd",
        "colab_type": "text"
      },
      "source": [
        "Not too shabby.\n",
        "\n",
        "Now we've seen how to implement the main training loop in pytorch. But we did almost everything else manually.\n",
        "\n",
        "Let's see what it would look like if we did the same thing but now using pytorch's built-in features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nk0vJF0y4ZY",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression with Pytorch -- the right way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nknfhNYtUCG2",
        "colab_type": "text"
      },
      "source": [
        "Instead of doing everything ourselves, we are going to take advantage of the many tools and methods that pytorch has built into it. This includes:\n",
        "- Dataset and Dataloader objects\n",
        "- Neural network layers\n",
        "- Optimizers\n",
        "- Various predefined loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zqre0MLWEap",
        "colab_type": "text"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfFefAbOWHSo",
        "colab_type": "text"
      },
      "source": [
        "First we need to import some stuff (we don't *have* to import these into our namespace, since they can be accessed from `torch.`, but it makes the calls shorter):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJA6n90TWH3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import tensor dataset & data loader\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CW7yeqNWhTY",
        "colab_type": "text"
      },
      "source": [
        "We can use the `TensorDataset` class to handle our data. One of the things this does is it lets us grab a row from the inputs and target as a tuple: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMRJxTZLW3BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define dataset\n",
        "train_ds = TensorDataset(inputs, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNmU0LAPWLtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can get rows:\n",
        "train_ds[0:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsw5-aawXGiJ",
        "colab_type": "text"
      },
      "source": [
        "Next, we will use pytorch's `DataLoader` which allows can split the data into batches when we train (and even shuffle or resample, if we want). This is useful because in many cases datasets are too big to process completely. Shuffling data when training is almost always a good idea and resampling is useful if our data is imbalanced in some way.\n",
        "\n",
        "We'll define the `batch_size` when create the DataLoader. This is just the number of rows of the data that we will train over before updating the parameters. Here we'll set it to be the length of the dataset (so that we only update parameters after processing all of the data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl6gBLaVWN-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define data loader\n",
        "batch_size = 2\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtfnr_FjXtkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We can see how shuffle works by running over this two times\n",
        "for i in range(0,2):\n",
        "  for xb,yb in train_dl:\n",
        "    print(xb,yb)\n",
        "\n",
        "# Now change the batch size in the prior cell and rerun both to see what's going on\n",
        "# Just be sure to set it back to 5 and rerun this before continuing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGbdQNlvX8gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next(iter(train_dl)) # this will return 5 (batch_size) rows of inputs and targets after shuffling. \n",
        "# iter() defines an iterator and next() tells python to get the next iteration from it."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5IMsoVfehaT",
        "colab_type": "text"
      },
      "source": [
        "We'll see why being able to change the order and group the data into different sized batches is important when we talk about the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0lA3GhgYiMR",
        "colab_type": "text"
      },
      "source": [
        "## Use a predefined linear layer -- nn.Linear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEiFF2UVYnQK",
        "colab_type": "text"
      },
      "source": [
        "Instead of:\n",
        "- defining weights and biases tensors manually\n",
        "- defining the linear function that uses weights nad biases to relate input to ouput via a matrix multiplication\n",
        "- initializing weights and biases to random values\n",
        "\n",
        "We can just use a layer that already does all that: ``torch.nn.Linear()``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LreLLFFlZIcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.nn.Linear(3,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtwyb54EZN6h",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy4jIfIAZPMH",
        "colab_type": "text"
      },
      "source": [
        "Instead of manually implementing Gradient Descent to adjust weights and biases according to the gradient and some learning rate, we can use one of Pytorch's built-in optimizers.\n",
        "\n",
        "Here we'll use the optimizer for <font color=blue>Stochastic Gradient Descent (SGD) </font>, but set it so that we're just doing regular <font color=blue>Gradient Descent</font>. The difference between the two is that in SGD, you adjust your weights and biases after running each item (row of data) through the network. It's **stochastic** because the data loader shuffles the data, so the order the data is seen varies from run to run. There is also another version, called <font color=blue>Batch Gradient Descent</font>, that falls somewhere in between the two -- you update parameters after running some batch of the data through the network. In pytorch, all of these versions (plus some variants) are implement with `torch.optim.SGD()`. Have a look at the [documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4T-EMI_b8aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5) # lr is the learning rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791rE2g2cGTY",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnB9aKvOcIQc",
        "colab_type": "text"
      },
      "source": [
        "Instead of using our manually defined `mse()` function, we'll use a predefined one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HjB8d5ScQJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = torch.nn.functional.mse_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0k9v371cZit",
        "colab_type": "text"
      },
      "source": [
        "So, for example, we could compute the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_974eJtDccfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = loss_fn(model(inputs),targets)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQ-aNX1cxPW",
        "colab_type": "text"
      },
      "source": [
        "## Define a fit function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZKNEVrkc0ZE",
        "colab_type": "text"
      },
      "source": [
        "Finally we will define a function to fit our model by putting all of the above pieces together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPWSHhMyc9x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt):\n",
        "    model.train() # put the model into training mode (in case it is already in evaluation mode)\n",
        "    for epoch in range(num_epochs):\n",
        "        for xb,yb in train_dl: # sample on batch of inputs,targets\n",
        "            opt.zero_grad() # tell the optimizer to zero the gradient so we start fresh on this batch\n",
        "            # Generate predictions and calculate loss\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            # Perform gradient descent\n",
        "            loss.backward()\n",
        "            opt.step() # this tells the optimizer to take one step -- it already knows about model parameters and the learning rate as we passed them when we create opt\n",
        "            \n",
        "    print('Training loss: ', loss_fn(model(inputs), targets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNi-2x64diFW",
        "colab_type": "text"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLYzKPtOdkm4",
        "colab_type": "text"
      },
      "source": [
        "Now we can train the model by running our fit function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBs31d7ndjrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model for 100 epochs\n",
        "fit(100, model, loss_fn, opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8n3WMSMdt4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predictions\n",
        "preds=model(inputs)\n",
        "\n",
        "# Plot predictions vs targets\n",
        "plt.plot(targets.detach().numpy(),preds.detach().numpy(),'.');\n",
        "plt.xlabel('targets');\n",
        "plt.ylabel('predictions');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU1XnCWSg2Er",
        "colab_type": "text"
      },
      "source": [
        "One important thing that we DID NOT DO in the above two examples is that we didn't split the data into training and test sets.  We could have accomplished this using:\n",
        "```\n",
        "train_ds,test_ds = torch.utils.data.random_split(full_ds,[len_train, len_test])\n",
        "```\n",
        "\n",
        "But we didn't do this because this was just for illustration purposes and the dataset we are using is pretty small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbI_xPeujHq-",
        "colab_type": "text"
      },
      "source": [
        "# Saving and Loading pytorch models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpConHg7jL9K",
        "colab_type": "text"
      },
      "source": [
        "You will want a way to save and (later to) load the models that you have trained. Pytorch has some tools to do this, using `torch.save()` and `torch.load()`.\n",
        "\n",
        "There are a few approaches:\n",
        "1. Save the entire model with `torch.save(model,path)` and then later load it using `model = torch.load(path)`\n",
        " - This will generate a save file that depends on the actual directory structure used in the specified path -- so you would only be able to load the save file if it was sitting in the same directory (even if you copied it to another system). So this way is not recommended. See the [pytorch docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for more info.\n",
        "2. Save the models state_dictionary only with `torch.save(model.state_dict(),path)`. When you want to load the model, you must then do the following:\n",
        "\n",
        "  ```python\n",
        "  model = Model() # make an instance of the model\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  ```\n",
        "\n",
        "3. Save a checkpoint that captures the model's state_dictionary and other state variables, such as the optimizer's state_dictionary, current epoch, and so on. This can be done by passing your own dictionary to `torch.save()` like this:\n",
        " \n",
        " ```python \n",
        " torch.save({'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            ...\n",
        "            },\n",
        "            path)\n",
        " ```\n",
        " \n",
        " Loading the model is then accomplished like this:\n",
        " \n",
        " ```python\n",
        " model = Model() # make an instance of the model\n",
        " checkpoint = torch.load(path)\n",
        " model.load_state_dict(checkpoint['model_state_dict'])\n",
        " ...\n",
        " opt.load_state_dict(checkpoint['opt_state_dict'])\n",
        " ...\n",
        " epoch = checkpoint['epoch']\n",
        " ```\n",
        "\n",
        "The 3rd way is the most bulletproof approach and the most configurable (you can add anything you want to the dictionary that you save -- including some text description). Unfortunately there is no \"standard file extension\" that is conventionally used, though it is common give torch saves the `.pt` or `.pth` file extension.  \n",
        "\n",
        "The consequence of this is: As long as you are consistent, you won't have issues saving and loading. But you may have trouble loading models that were saved by others, so be aware of this.\n",
        "\n",
        "**One thing to note: colab won't save your files if your session disconnects, so you should download them after you save them.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knWrJSKJLUq0",
        "colab_type": "text"
      },
      "source": [
        "# Define a simple Linear NN and train it to predict..."
      ]
    }
  ]
}