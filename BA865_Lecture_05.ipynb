{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA865 - Lecture 05.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/BA865/blob/master/BA865_Lecture_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlilQzLYR8Ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0gnVvqjR8Jz",
        "colab_type": "text"
      },
      "source": [
        "# scikit-learn: Machine Learning in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8GEwbJaR8J0",
        "colab_type": "text"
      },
      "source": [
        "Numerous machine learning algorithms and models are implemented in [scikit-learn](https://scikit-learn.org/stable/index.html).\n",
        "\n",
        "Six main categories are\n",
        "* Regression\n",
        "* Classification\n",
        "* Clustering\n",
        "* Dimensionality reduction\n",
        "* Model selection\n",
        "* Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3oGKRHzR8J1",
        "colab_type": "text"
      },
      "source": [
        "## Estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBF6-bVqR8J1",
        "colab_type": "text"
      },
      "source": [
        "In scikit-learn, machine learning model is called as **Estimator**.\n",
        "\n",
        "Each **Estimator** is a Python `class` and has a form like (Recall the structure of `class`).\n",
        "\n",
        "```python\n",
        "class estimator():\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def fit():\n",
        "        # do some calculations with self.data\n",
        "```\n",
        "\n",
        "So if you want to estimate coefficients or learn patterns within data, simply do\n",
        "\n",
        "1. initialize an estimator\n",
        "2. Fit the estimator with data of your interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfwuCW0NR8J2",
        "colab_type": "text"
      },
      "source": [
        "## Regression : Linear Regression - Ordinary Least Squares (OLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Az9FIrpR8J3",
        "colab_type": "text"
      },
      "source": [
        "Ordinary Least Squares\n",
        "\n",
        "$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p$\n",
        "\n",
        "Estimate $w$ that minimizes $\\sum{(y-Xw)^2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWbhi9l6R8J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize a linear model estimator\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tPmFjwZR8J5",
        "colab_type": "text"
      },
      "source": [
        "`LinearRegression` accepts two inputs X and Y.\n",
        "\n",
        "Their formats should be organized as below.\n",
        "\n",
        "X = $[[x_{11}, x_{12}], [x_{21}, x_{22}], [x_{31},x_{32}]]$\n",
        "\n",
        "y = $[y_{1}, y_{2}, y_{3}]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDpDz-U5R8J6",
        "colab_type": "text"
      },
      "source": [
        "In case that $x_i$ has only one value, let $X = [[x_{11}], [x_{21}], [x_{31}]]$.\n",
        "\n",
        "So, if we want to estimate a linear model for the data $X=[1,2,3,4,5]$ and $y=[0,2,4,1,4]$, change the format of X as $[[1], [2], [3], [4], [5]]$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYBR3aK0R8J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [0,2,4,1,4]\n",
        "plt.scatter(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdv9PmxIR8J9",
        "colab_type": "text"
      },
      "source": [
        "Now fit the estimator `lm`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU1syj2vR8J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp-GPxWkR8KA",
        "colab_type": "text"
      },
      "source": [
        "Estimated intercept $w_0$ and coefficients $w_i$ are stored in `lm.intercept_` and `lm.coef_` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlZ_x1JnR8KB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Estimated intercept is', lm.intercept_, 'and estimated coefficient is', lm.coef_[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpCeydKLR8KD",
        "colab_type": "text"
      },
      "source": [
        "The estimated linear model is $Y = 0.1 + 0.7X$.\n",
        "\n",
        "Do you want to predict a value for $x=10$? &rarr; use `lm.predict()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrYCT6NdR8KE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm.predict([[10]]) # be sure that X should be given as two dimensional array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id-UZrCXR8KG",
        "colab_type": "text"
      },
      "source": [
        "Lastly, using `lm.predict()`, we can draw the estimated line with data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FpFo978R8KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = lm.predict(X)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='gray', linestyle='--')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRhpw5_UR8KI",
        "colab_type": "text"
      },
      "source": [
        "Remember the rules of scikit-learn. Initialize and fit.\n",
        "\n",
        "<span style=\"color:red\"> **Exercises with kaggle data?**</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrGa40KNR8KJ",
        "colab_type": "text"
      },
      "source": [
        "## Classification : Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7D-9PFRR8KJ",
        "colab_type": "text"
      },
      "source": [
        "Support vector machine (SVM) is a supervised learning model that classifies data points into given labels.\n",
        "SVM finds hyperplanes that maximally divide labels and uses the hyperplanes as classifiers. \n",
        "\n",
        "* For $p$ dimensional vectors, its hyperplane of $(p-1)$ dimensions can separate the vectors into labels.\n",
        "* For example, if each observation has two values, a hyperplane that divides observations is a line (1-dim).\n",
        "* If each observation has three values, a hyperplane is a plane (2-dim).\n",
        "\n",
        "A hyperplane that divides data points can be expressed as $\\overrightarrow{w}$ that satisfies $\\overrightarrow{w}\\overrightarrow{x}-b = c$ where $c$ is a value between two labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo2IfCviR8KK",
        "colab_type": "text"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Exi3DUDR8KK",
        "colab_type": "text"
      },
      "source": [
        "Randomly generate samples from two multivariate normal distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXVCtb0ZR8KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGG37GGcR8KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat1[:,0], dat1[:,1])\n",
        "plt.scatter(dat2[:,0], dat2[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhDLSDTCR8KR",
        "colab_type": "text"
      },
      "source": [
        "1. Initialize an estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05wSNi9mR8KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC() # Linear support vector machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvWqLzJVR8KU",
        "colab_type": "text"
      },
      "source": [
        "2. Fit the estimator to the data. We will use age and fare to learn the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UqobHtOR8KV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((dat1, dat2))\n",
        "y = [0]*50 + [1]*50\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoi14cycR8KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(clf.intercept_, clf.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwJ74UiuR8Ka",
        "colab_type": "text"
      },
      "source": [
        "Thus, the learned classifier is $-2.87 + 1.30x_{1}+0.67x_{2}$.\n",
        "\n",
        "Predict labels of other data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJBBi3IFR8Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pred = np.array([[0.5, 0], [1.5, 3], [3, 2]])\n",
        "y_pred = clf.predict(X_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNBJqj2DR8Ke",
        "colab_type": "text"
      },
      "source": [
        "#### Check the results on a plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBNQykFjR8Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat1[:,0], dat1[:,1], alpha=0.2)\n",
        "plt.scatter(dat2[:,0], dat2[:,1], alpha=0.2)\n",
        "\n",
        "X_tmp = np.arange(0.5, 2.5, 0.1) \n",
        "SVM_line = 1/clf.coef_[0][1]*(-clf.intercept_[0] - clf.coef_[0][0]*X_tmp)\n",
        "plt.plot(X_tmp, SVM_line, color='gray', linestyle='--', label='SVM')\n",
        "\n",
        "\n",
        "plt.scatter(X_pred[:,0], X_pred[:,1], marker='s', s=100, \n",
        "            color = ['tab:blue' if x==0 else 'tab:orange' for x in y_pred], label='Predicted')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5MVvMRR8Ki",
        "colab_type": "text"
      },
      "source": [
        "### Q. Can we build a classifier that predicts who survived the Titanic? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oh9nhUOR8Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "titanic=sns.load_dataset('titanic') # load data\n",
        "titanic.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PceDZmsiR8Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanic.sex = [0 if x=='male' else 1 for x in titanic.sex] # male = 0, female = 1\n",
        "X = titanic[['pclass', 'sex', 'age', 'fare']] # use four columns\n",
        "not_na = ~pd.isna(X.age)\n",
        "X = X[not_na]\n",
        "y = titanic['survived'][not_na] # survived is the label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcQ00btkR8Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBvFVFLAR8Kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA6wMrgRR8Kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(X)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DRqU7DRR8K0",
        "colab_type": "text"
      },
      "source": [
        "## Model selection : How to improve and evaluate the learned model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgjUj7BpR8K1",
        "colab_type": "text"
      },
      "source": [
        "Until now, we used the entire data to learn a model. It is in fact wrong! We need to test the learned model for new data.\n",
        "\n",
        "&rarr; split data into train, validation, and test sets.\n",
        "\n",
        "* Train set: a subset of data to train a model\n",
        "* Validation set: a subset of data to tune hyperparameters\n",
        "* Test set: a subset of data to evaluate the learned model\n",
        "\n",
        "As our current goal is not to tune hyperparameters for better performance, we will use 70\\% of data as train set and 30\\% as test set.  \n",
        "\n",
        "Fortunately, `sklearn` provides an easy way to split data into train and test sets.\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQENs5YvR8K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FFVt_IdR8K4",
        "colab_type": "text"
      },
      "source": [
        "Now learn a SVM model for the train sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0_D8epR8K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYjgZx3R8K6",
        "colab_type": "text"
      },
      "source": [
        "Then predict labels for the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeKX4jexR8K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogDZbwPdR8K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cF24XcVR8K_",
        "colab_type": "text"
      },
      "source": [
        "## Clustering : K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvgUzgpUR8LB",
        "colab_type": "text"
      },
      "source": [
        "What if no labels are given in data? \n",
        "\n",
        "Classifying data by underlying patterns is needed, and computational models do this job are called **unsupservised learning**. K-means clustering is one of popular unsupervised learning methods.\n",
        "\n",
        "Simple idea: Assume that data points are separated into $K$ clusters. Find $K$ centroids that minimize witin-cluster variances\n",
        "\n",
        "Let $(x_1, x_2, ..., x_n)$ be observations and $(\\mu_1, \\mu_2, ..., \\mu_m)$ be centroids of points in cluster $i, C_i$.\n",
        "\n",
        "Minimize $\\sum_{i=1}^{m}\\sum_{x\\in C_{i}}\\|x-\\mu_i\\|^2$.\n",
        "\n",
        "<span style=\"color:red\"> **The number of clusters $K$ should be given**</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k4T5lo7R8LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat = np.concatenate((dat1, dat2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9hl0I2gR8LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat[:,0], dat[:,1], color='gray') # but it is originally generated by two different distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JutSlL_wR8LF",
        "colab_type": "text"
      },
      "source": [
        "### K=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCyDEfn3R8LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans.fit(dat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5t7X4epR8LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(dat1[:,0], dat1[:,1])\n",
        "plt.scatter(dat2[:,0], dat2[:,1])\n",
        "plt.title('Original data')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "labels = kmeans.labels_\n",
        "plt.scatter(dat[:,0], dat[:,1], color=['tab:blue' if x==0 else 'tab:orange' for x in labels])\n",
        "plt.title('K-means clustering: K=2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4BLo5tNR8LM",
        "colab_type": "text"
      },
      "source": [
        "### K=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpyGtBAIR8LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(dat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRcSknSvR8LO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(dat1[:,0], dat1[:,1])\n",
        "plt.scatter(dat2[:,0], dat2[:,1])\n",
        "plt.title('Original data')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "labels = kmeans.labels_\n",
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(dat[:,0], dat[:,1], color=[cmap[x] for x in labels])\n",
        "plt.title('K-means clustering: K=4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV3N3StJR8LP",
        "colab_type": "text"
      },
      "source": [
        "### Example: wine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hji4sYTR8LQ",
        "colab_type": "text"
      },
      "source": [
        "In this data, chemical compositions and types of wines are given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jaQf6fwR8LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzLP2Lb6R8LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej7MKudHR8LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSTJKaZWR8LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYBS0PcKR8La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = kmeans.predict(X_test)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R7hvbg2R8Lc",
        "colab_type": "text"
      },
      "source": [
        "Not bad, but there is room for improvement. How?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_igSbaNZR8Lc",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing : Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yyYqZskR8Ld",
        "colab_type": "text"
      },
      "source": [
        "Standardization is required for data like the wine case because values are distributed with different means and variances by columns. These characteristics would affect performance and implementation of machine learning algorithms. By standardizing, we can resolve this issue to some extent. To standardize data, we will use the `sklearn.preprocessing` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eql5ceLWR8Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() # calculate mean and standard deviation of train set\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCSm8ka1R8Le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler.mean_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wh3GcgOR8Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler.scale_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTvf6IaCR8Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z92brdknR8Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBQcvcJTR8Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDJNgySCR8Ln",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality reduction : Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGJaNhIhR8Lo",
        "colab_type": "text"
      },
      "source": [
        "Wine data has 13 features that makes us hard to visualize and understand the data. Through dimensionality reduction, we can get following advantages. (https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
        "* It reduces the time and storage space required.\n",
        "* Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model.\n",
        "* It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
        "* It avoids the curse of dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQIymr37R8Lo",
        "colab_type": "text"
      },
      "source": [
        "What is PCA?\n",
        "\n",
        "Eigenvalue, eigenvectors? Hard mathematics. <span style=\"color:blue\"> **HK: Do we need to explain details?**</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIBqPSA5R8Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled) # find principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX-DB73SR8Lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBmGD3CqR8Ls",
        "colab_type": "text"
      },
      "source": [
        "About 37\\% variances are explained by the first principle component and about 19\\% variances are explained by the second principle component. It means that the first two components capture more than half of all variances. \n",
        "\n",
        "So, projecting the wine data onto the first two principal components can give a good overview of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhQC6XINR8Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_transformed = pca.transform(X_train_scaled) # project the data onto principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaV0n80VR8Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(pca_transformed[:,0], pca_transformed[:,1], color = [cmap[x] for x in y_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48EnjBwDR8Lw",
        "colab_type": "text"
      },
      "source": [
        "Three wine types are separated well by the first two principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBkEvI8R8Lx",
        "colab_type": "text"
      },
      "source": [
        "## Pipelines : chaining pre-processors and estimators\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4H53lh2R8Lx",
        "colab_type": "text"
      },
      "source": [
        "We can do all procedures at once by the `sklearn.pipeline` package!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fAFCcVtR8Ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "Y = wine.target # there are three types\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0)\n",
        "\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    KMeans(n_clusters=3, random_state=0)\n",
        ")\n",
        "\n",
        "pipe.fit(X_train, Y_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "Y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(accuracy_score(Y_test, Y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}