{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA865_Lecture_05.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/BA865/blob/master/BA865_Lecture_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swByiBfML8m_",
        "colab_type": "text"
      },
      "source": [
        "# Code Preface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTfOe5Ymb_Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvfQRYqvb_Z5",
        "colab_type": "text"
      },
      "source": [
        "# scikit-learn: Machine Learning in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVV6S587b_Z_",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=19uME3dIsvBDOtVg5eDeIN7tmLLiRpjwx\" width=300>\n",
        "\n",
        "\n",
        "Scikit Learn (sklearn for short) is a machine learning library in Python that provides access to many common [ML algorithms](https://scikit-learn.org/stable/index.html). You've already seen supervised and unsupervised ML algorithms. So in this lecture, we'll focus on the basics of the sklearn APIs.  \"scikit\" refers to \"scipy toolkit\". \n",
        "\n",
        "The six main categories of scikit-learn algorithms are:\n",
        "* Regression\n",
        "* Classification\n",
        "* Clustering\n",
        "* Dimensionality reduction\n",
        "* Model selection\n",
        "* Preprocessing\n",
        "\n",
        "Data (features and targets) is passed into sklearn algorithms typically as Pandas dataframes or numpy arrays or even python lists. \n",
        "\n",
        "\n",
        "We'll start by talking about the Estimator API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nJTFOYlb_aH",
        "colab_type": "text"
      },
      "source": [
        "## Estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3i2SEe6b_aN",
        "colab_type": "text"
      },
      "source": [
        "In scikit-learn, a machine learning model is called as **Estimator**.\n",
        "\n",
        "Each **Estimator** is a Python `class` and has a form like this:\n",
        "\n",
        "```python\n",
        "class estimator():\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def fit():\n",
        "        # do some calculations with self.data\n",
        "```\n",
        "\n",
        "Most commonly, the steps in using the Scikit-Learn Estimator API are as follows:\n",
        "\n",
        "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
        "2. Choose model hyperparameters by instantiating this class with desired values.\n",
        "3. Arrange data into a features matrix and target vector.\n",
        "4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n",
        "5. Apply the Model to new data:\n",
        "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
        "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyDqKd7jPam8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "So if you want to estimate coefficients or learn patterns within data, simply do the following:\n",
        "\n",
        "1. initialize an estimator.\n",
        "2. Fit the estimator with data of your interest.\n",
        "\n",
        "We'll walk through some very common ML algorithms, starting with regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efFIkl6b_aS",
        "colab_type": "text"
      },
      "source": [
        "## Regression : Linear Regression - Ordinary Least Squares (OLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2TON8pjb_aU",
        "colab_type": "text"
      },
      "source": [
        "You should all be familiar with the concept of regression. For Ordinary Least Squares regression, a model is written as:\n",
        "\n",
        "$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p$\n",
        "\n",
        "where $\\hat{y}$ is the target and $x_i$ are a set of features. Learning the model is all about finding a set of weights $w_i$ that minimizes: \n",
        "\n",
        "$\\sum{(y-Xw)^2}$\n",
        "\n",
        "To accomplish this in sklearn, we'll start by importing `linear_model` and initializing it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xyPomib_aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize a linear model estimator\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FVlKQ6Yb_aj",
        "colab_type": "text"
      },
      "source": [
        "`LinearRegression` accepts two inputs X and Y.\n",
        "\n",
        "Their formats should be organized as below.\n",
        "\n",
        "X = $[[x_{11}, x_{12}], [x_{21}, x_{22}], [x_{31},x_{32}]]$\n",
        "\n",
        "y = $[y_{1}, y_{2}, y_{3}]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M31wxlV5b_ao",
        "colab_type": "text"
      },
      "source": [
        "In case that $x_i$ has only one value, let $X = [[x_{11}], [x_{21}], [x_{31}]]$.\n",
        "\n",
        "So, if you want to estimate a linear model for the data $X=[1,2,3,4,5]$ and $y=[0,2,4,1,4]$, you should change the format of X as $[[1], [2], [3], [4], [5]]$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pg_t03Fb_ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [0,2,4,1,4]\n",
        "plt.scatter(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz8cak8zb_a0",
        "colab_type": "text"
      },
      "source": [
        "Now fit the estimator `lm`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xihWeA-b_a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imNo2r_eb_a-",
        "colab_type": "text"
      },
      "source": [
        "Estimated intercept $w_0$ and coefficients $w_i$ are stored in `lm.intercept_` and `lm.coef_` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WphayXjb_bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Estimated intercept is {lm.intercept_} and estimated coefficient is {lm.coef_[0]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcba9Obzb_bI",
        "colab_type": "text"
      },
      "source": [
        "The estimated linear model is $Y = 0.1 + 0.7X$.\n",
        "\n",
        "Once we have fitr the model we'll use the model's `predict()` method to get a prediction for new data.\n",
        "\n",
        "For example, if you want to predict a value for $x=10$? &rarr; we can use `lm.predict()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyTqGrdfb_bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm.predict([[10]]) # be sure that X should be given as two dimensional array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o-k1cbub_bT",
        "colab_type": "text"
      },
      "source": [
        "Lastly, we can use `lm.predict()`, to draw the estimated based on the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JHMmMD4b_bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = lm.predict(X)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='gray', linestyle='--')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5idZnEJxb_bd",
        "colab_type": "text"
      },
      "source": [
        "Remember the rules of scikit-learn. Initialize and fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZFtG5Tb_bf",
        "colab_type": "text"
      },
      "source": [
        "## Classification : Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii6xOeQUb_bj",
        "colab_type": "text"
      },
      "source": [
        "A Support Vector Machine (SVM) is a supervised learning model that classifies data points into a set of given classes or labels.\n",
        "\n",
        "SVM finds \"hyperplanes\" (think of the higher dimensional extension of a line dividing two regions of space in two dimension) that maximally divide labels and uses the hyperplanes as classifiers. \n",
        "\n",
        "* For $p$ dimensional vectors, its hyperplane of $(p-1)$ dimensions can separate the vectors into labels.\n",
        "* For example, if each observation has two values, a hyperplane that divides observations is a line (1-dim).\n",
        "* If each observation has three values, a hyperplane is a plane (2-dim).\n",
        "\n",
        "A hyperplane that divides data points can be expressed as $\\overrightarrow{w}$ that satisfies $\\overrightarrow{w}\\overrightarrow{x}-b = c$ where $c$ is a value between two labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAJhuUDb_bl",
        "colab_type": "text"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RmvBbM3b_bn",
        "colab_type": "text"
      },
      "source": [
        "To illustrate this, we'll create an artificial dataset by randomly generating samples from two multivariate normal distributions. The \"class\" or \"label\" in this case is the distribution from which it is drawn (in this example, there are two different distributions that we draw points from, so there are two classes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TYXxWxJb_bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1) # set the seed, so we all get the same random draws and therefore have the same data.\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blw0UIeab_bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the datapoints drawn from these two distributions\n",
        "plt.scatter(dat1[:,0], dat1[:,1]);\n",
        "plt.scatter(dat2[:,0], dat2[:,1]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2xLy2Nb_b0",
        "colab_type": "text"
      },
      "source": [
        "To proceed, we simply follow the steps, starting with:\n",
        "\n",
        "1. Initialize an estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKA4iFLnb_b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC() # There are different types of SVMs, here we'll use a Linear support vector machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRSb9qudb_b_",
        "colab_type": "text"
      },
      "source": [
        "2. Fit the estimator to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8PrqkZyb_cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((dat1, dat2))\n",
        "y = [0]*50 + [1]*50 # This is just a quick way to make the labels by repeating the elements of two lists and concatenating them together. \n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfEdcQ-4SuuT",
        "colab_type": "text"
      },
      "source": [
        "To see what the model arrived at for the fit, we can print the coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoDf07-eb_cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(clf.intercept_, clf.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P78uDG0pb_cS",
        "colab_type": "text"
      },
      "source": [
        "Thus, the learned classifier is $-2.87 + 1.30x_{1}+0.67x_{2}$.\n",
        "\n",
        "And we can make predictions using this model for new data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv2mxrlbb_cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pred = np.array([[0.5, 0], [1.5, 3], [3, 2]])\n",
        "y_pred = clf.predict(X_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmNBNVDeb_ca",
        "colab_type": "text"
      },
      "source": [
        "#### Check the results on a plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqgOJV4_TBTP",
        "colab_type": "text"
      },
      "source": [
        "Let's visualise the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdrs_uMzb_ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat1[:,0], dat1[:,1], alpha=0.2);\n",
        "plt.scatter(dat2[:,0], dat2[:,1], alpha=0.2);\n",
        "\n",
        "X_tmp = np.arange(0.5, 2.5, 0.1) # generate tickpoints along the x axis, so we can use them to draw the estimated line\n",
        "SVM_line = 1/clf.coef_[0][1]*(-clf.intercept_[0] - clf.coef_[0][0]*X_tmp) # get the dividing line\n",
        "plt.plot(X_tmp, SVM_line, color='gray', linestyle='--', label='SVM');\n",
        "\n",
        "\n",
        "plt.scatter(X_pred[:,0], X_pred[:,1], marker='s', s=100, \n",
        "            color = ['tab:blue' if pred==0 else 'tab:orange' for pred in y_pred], label='Predicted'); # plot the predicted data for our three new datapoints\n",
        "\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRExyPGEb_co",
        "colab_type": "text"
      },
      "source": [
        "### Example: Let's build a SVM classifier that predicts who survived the Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWChR1-Db_cq",
        "colab_type": "text"
      },
      "source": [
        "We'll start by looking at some data on who survived the titanic. The full dataset is included as part of the `seaborn` package, but we'll use a version that I've reduced and cleaned up a bit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK8q7MAwb_ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titanicFile='https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/titanic_cleaned.csv'\n",
        "titanic = pd.read_csv(titanicFile)\n",
        "titanic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfAaLpMbb_c0",
        "colab_type": "text"
      },
      "source": [
        "We'll build a model using the pclass (passenger class), sex, age, and fare (the fair they paid) to try to predict survived (whether the passenger survived)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6h03xHNb_dM",
        "colab_type": "text"
      },
      "source": [
        "First, we'll initiate an SVM model. Unlike the previous example, we'll use the SVC type that provides some advanced kernels. I won't talk about these in detail, though you can read about SVM kernels [here](https://data-flair.training/blogs/svm-kernel-functions/), except to say that nonlinear kernels permit finding divisions in the data beyond a simple line dividing the space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsEkAsNsb_dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd2qqqk1YzQR",
        "colab_type": "text"
      },
      "source": [
        "In order to evaluate an ML model, there are a variety of metrics that you should already be aware of (e.g., accuracy, precision, recall, F-score, etc.). We'll use accuracy, which we can import from `sklearn.metrics`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QB4tdR6b_dT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(X)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS0eATPsZE7S",
        "colab_type": "text"
      },
      "source": [
        "So you might think this model performs pretty well. However, this is not a good assessment of the model, because we assessed performance on the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2mxsUwZb_dZ",
        "colab_type": "text"
      },
      "source": [
        "## Model selection : How to improve and evaluate the learned model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkw89ZJbb_df",
        "colab_type": "text"
      },
      "source": [
        "Up until this point, we used the entire data to learn the model. However, you know that this is not the correct approach. We need to holdout some data in order to evaluate the model that we learned on \"unseen data\".\n",
        "\n",
        "A typical step after loading data is to split it into train, validation, and test sets:\n",
        "\n",
        "* Train set: a subset of data to train a model\n",
        "* Validation set: a subset of data to evaluate the model\n",
        "* Test set: a subset of data to evaluate the learned model\n",
        "\n",
        "As our current goal is not to evaluate and build a model for better performance we'll just split the data into a 70\\% training set and 30\\% as test set.  \n",
        "\n",
        "Fortunately, `sklearn` provides an easy way to split data into train and test sets, using `train_test_split()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZf29Os5b_di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Let 30% of the data to be a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgtHuRiPb_dp",
        "colab_type": "text"
      },
      "source": [
        "Now we'll learn an SVM model, using only the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8JaPiFOb_dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGorV5GZb_dx",
        "colab_type": "text"
      },
      "source": [
        "We can then get the predicted labels for the test set using the model's `predict()` method and then test the accuracy (out of sample performance):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWADQ9Vub_d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDTYKI_zb_d6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faSBt761afjb",
        "colab_type": "text"
      },
      "source": [
        "SO the model performs **okay** (50% accuracy would be achieved with random prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_1ae1tob_eD",
        "colab_type": "text"
      },
      "source": [
        "## Clustering : K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hBZWTobb_eF",
        "colab_type": "text"
      },
      "source": [
        "What if we have data where no labels are given ? \n",
        "\n",
        "One class of ML models discovers finds clusters in the data based on patterns. This process is called **unsupervised learning**. K-means clustering is one type of popular unsupervised learning methods.\n",
        "\n",
        "Here's the basic idea: Assume that data points belong to one of $K$ different clusters. Our task is to find $K$ different centroids (each is the center of the cluster) and assign datapoints to one of these $K$ clusters, such that the  within-cluster variances are minimized.\n",
        "\n",
        "Let $(x_1, x_2, ..., x_n)$ be observations and $(\\mu_1, \\mu_2, ..., \\mu_m)$ be centroids of points in cluster $i, C_i$.\n",
        "\n",
        "Then, our task is to minimize:\n",
        "\n",
        " $\\sum_{i=1}^{m}\\sum_{x\\in C_{i}}\\|x-\\mu_i\\|^2$.\n",
        "\n",
        "<font color=red> **We have to specify the number of clusters $K$ at the beginning**</font>\n",
        "\n",
        "Just as before, we'll start by generating synthetic data by drawing datapoints from different distributions. We'll use the exact same data as in the SVM example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWAnF8gJb_eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The same data that are generated in the SVM example\n",
        "np.random.seed(1)\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat = np.concatenate((dat1, dat2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdbPoXy3b_eM",
        "colab_type": "text"
      },
      "source": [
        "Assume that we do not know underlying clusters to which data points belong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNoJhDw-b_eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat[:,0], dat[:,1], color='gray'); # but it is originally generated by two different distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-z7tXIcQaw",
        "colab_type": "text"
      },
      "source": [
        "Let's try it out for different values of $K$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozlD2gbHb_eU",
        "colab_type": "text"
      },
      "source": [
        "### K=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZmvDNP4b_eX",
        "colab_type": "text"
      },
      "source": [
        "Set n_clusters=2 in the function KMeans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX3ZYyAlb_eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0) \n",
        "kmeans.fit(dat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Ckk3VWb_eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,4));\n",
        "plt.subplot(1,2,1);\n",
        "plt.scatter(dat1[:,0], dat1[:,1]);\n",
        "plt.scatter(dat2[:,0], dat2[:,1]);\n",
        "plt.title('Original data');\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "labels = kmeans.labels_\n",
        "plt.scatter(dat[:,0], dat[:,1], color=['tab:orange' if x==0 else 'tab:blue' for x in labels]);\n",
        "plt.title('K-means clustering: K=2');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23yzuOBb_en",
        "colab_type": "text"
      },
      "source": [
        "You can see that K-means clustering did a fairly good job at detecting the clusters that we created synthetically. Of course, since this is unsupervised learning, we would now the \"true\" clusters or classes that the data belonged to -- so we can't assess the performance directly.\n",
        "\n",
        "\n",
        "We would also have no way of knowing whether the data is best characterized by two clusters or more. So let's have a look at other values of $K$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCKecUw5b_ep",
        "colab_type": "text"
      },
      "source": [
        "### K=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alAbjo-Mb_es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(dat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQQWOc4Nb_e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(dat1[:,0], dat1[:,1]);\n",
        "plt.scatter(dat2[:,0], dat2[:,1]);\n",
        "plt.title('Original data');\n",
        "\n",
        "plt.subplot(1,2,2);\n",
        "labels = kmeans.labels_\n",
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(dat[:,0], dat[:,1], color=[cmap[x] for x in labels]);\n",
        "plt.title('K-means clustering: K=4');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2YG0rk9b_e_",
        "colab_type": "text"
      },
      "source": [
        "The algorithm classifies data points for a given number of clusters, whatever the number is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPH7_xJb_fB",
        "colab_type": "text"
      },
      "source": [
        "### Example: Unsupervised clustering applied to wine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5izbjLZb_fE",
        "colab_type": "text"
      },
      "source": [
        "In this data, chemical compositions and types of wines are given. There are three types of wines (i.e., coming from different winemakers). We will investigate the extent to which k-means clustering can recover the wine types.\n",
        "\n",
        "Note that there isn't any correlation between the label kmeans ascribes and the true label of the wine (on one run, a particular type of wine might be represented with cluster labeled by 1, but on another the same approximate cluster might be assigned the label 2). Because of this, we'll need a metric that is not dependent on the actual value of the labels. A good metric is the [Mutual Information Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html) which is defined by:\n",
        "\n",
        " $MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}$\n",
        "\n",
        "where $|U_i|$ is the number of samples in cluster $U_i$ and $|V_j|$ is the number of samples in cluster $V_j$. We'll use the predicted labels and true labels as $U$ and $V$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYChin8sb_fG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import mutual_info_score\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oijMWQ0Wb_fR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKosMynnb_fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oQWzWjxb_ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_63XiqXob_fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = kmeans.predict(X_test)\n",
        "print('{:.2%}\\n'.format(mutual_info_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXgTmnmMb_fo",
        "colab_type": "text"
      },
      "source": [
        "Not bad, but there is room for improvement. How?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja0Npxxyb_fp",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing : Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh0UA6vqb_ft",
        "colab_type": "text"
      },
      "source": [
        "Often you will encounter data where the features have very different distributions (with very different mean and variance for each column). In such a case, you can benefit from **standardization** -- a process of transforming the features so that each column has zero mean and unit variance. Some ML algorithms will suffer if the data on which they are trained is not standardized. To standardize data, we will use the `sklearn.preprocessing` package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWgddIab_fu",
        "colab_type": "text"
      },
      "source": [
        "`StandardScaler` calculates mean and standard devaition of a train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge9kAroqb_fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() # calculate mean and standard deviation of train set\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aY3ED_7b_fz",
        "colab_type": "text"
      },
      "source": [
        "Let's print the means of the different features (columns):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFlWJanhb_f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler.mean_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSQLQr4Xb_f6",
        "colab_type": "text"
      },
      "source": [
        "`scaler.scale_` returns the standard deviations for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oshs4FUb_f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler.scale_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDO2KYTb_f-",
        "colab_type": "text"
      },
      "source": [
        "To standardize the data, we simply call `scaler.transform` on the original values to get the transformed data. Transformed data will have 0 mean and 1 variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5n5eXaSb_gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJuv-qkXb_gD",
        "colab_type": "text"
      },
      "source": [
        "Let's check means of transformed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OyiMbZb_gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(X_train_scaled, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QKMgBrbb_gI",
        "colab_type": "text"
      },
      "source": [
        "Compared to means of the original data, those of the standardized data are close to zero. Now, train a k-mean clustering model with the standardized data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ7bvzYHb_gJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5lCdOf5b_gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(mutual_info_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gqZ2uKtb_gO",
        "colab_type": "text"
      },
      "source": [
        "You can see that standardizing the data has a pretty substantial impact on the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWjah-Fb_gP",
        "colab_type": "text"
      },
      "source": [
        "### Pipelines : chaining pre-processors and estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0vpFxmb_gP",
        "colab_type": "text"
      },
      "source": [
        "We can do all procedures (standardize data and learn a model) at once by taking advantage of the `sklearn.pipeline` package!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzvouLKib_gQ",
        "colab_type": "text"
      },
      "source": [
        "The below code block shows how to combine the different operations, scaling with the `StandardScaler` and running the `KMeans` algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSfp44tb_gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    KMeans(n_clusters=3, random_state=0)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG_FnXyxb_gT",
        "colab_type": "text"
      },
      "source": [
        "To execute this pipe, following the rules of sklearn, use `.fit` method.\n",
        "```python\n",
        "pipe.fit(X_train, Y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFhoHS09b_gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "Y = wine.target # there are three types\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0)\n",
        "\n",
        "pipe.fit(X_train, Y_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "Y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(mutual_info_score(Y_test, Y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAJ3WuS4b_ge",
        "colab_type": "text"
      },
      "source": [
        "And of course we get the same result as before.  Making a pipeline is useful as frequently you'll want to do several transformations of your data as part of model fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLsakRUb_gf",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs-21vkrb_gg",
        "colab_type": "text"
      },
      "source": [
        "Dimensionality reduction offers several advantages. (https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
        "* It reduces the time and storage space required.\n",
        "* Removes  multi-collinearity to improve the interpretation of the parameters of the machine learning model.\n",
        "* It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
        "* It avoids the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uv3YvAb_gh",
        "colab_type": "text"
      },
      "source": [
        "For example, we can reduce 13 dimensions in the wine data into 2 dimensions. This process helps to understand and visualize complicated data. In this section, we will cover a popular dimensionality reduction method: **principal component analysis (PCA)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbiiqFdb_gi",
        "colab_type": "text"
      },
      "source": [
        "#### What is PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnvWcA0Tb_gj",
        "colab_type": "text"
      },
      "source": [
        "PCA finds linearly uncorrelated variables by combining existing correlated variables. Let's explore the concept with the wine data. In the wine data, \"alcohol\" and \"color_intensity\" are correlated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvHMReNUb_gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYl8rU5b_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,-4])\n",
        "plt.xlabel(\"Alcohol (standardized)\")\n",
        "plt.ylabel(\"Color intensity (standardized)\")\n",
        "print(\"Correlation between alchol and color intensity is\", round(X.alcohol.corr(X.color_intensity), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ABrRX9b_gs",
        "colab_type": "text"
      },
      "source": [
        "As the two variables are correlated, significant amount of variances between them can be captured through a new variable. PCA returns this new variable by combining correlated ones. The new variable is represented as the arrow on the below plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fx4Rkxb_gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled[:, (0,-4)]) # find principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K_q_VDYb_g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6));\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,-4], alpha=0.3);\n",
        "plt.xlabel(\"Alcohol (standardized)\");\n",
        "plt.ylabel(\"Color intensity (standardized)\");\n",
        "plt.annotate(\"\", [0,0], -3*pca.explained_variance_ratio_[0]*pca.components_[:,0], \n",
        "             arrowprops=dict(arrowstyle='<-', linewidth=3, color='red'));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koeqZmKCb_g9",
        "colab_type": "text"
      },
      "source": [
        "In this way, PCA finds a given number of components (`n_components`) that are uncorrelated and explain variance well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-MalDzcb_g-",
        "colab_type": "text"
      },
      "source": [
        "\"pca.explained_variance_ratio_\" summarizes how much variance that a component explains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-QtCkxob_hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ebLePtb_hC",
        "colab_type": "text"
      },
      "source": [
        "This shows that the first principal component (red arrow) explains about 78% of the total variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBTMrmXIb_hD",
        "colab_type": "text"
      },
      "source": [
        "Let's apply PCA for the entire wine data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ujm6qzb_hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled) # find principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-IwUWDhb_hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VklOxnlb_hO",
        "colab_type": "text"
      },
      "source": [
        "About 37\\% of the variance is explained by the first principle component and about 19\\% of the variance is explained by the second principle component. It means that the first two components capture more than half of all the variance of the data. \n",
        "\n",
        "So, projecting the wine data onto the first two principal components can give a good overview of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h63-M3a-b_hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_transformed = pca.transform(X_train_scaled) # project the data onto principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MSsVmNQb_hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(pca_transformed[:,0], pca_transformed[:,1], color = [cmap[x] for x in y_train]);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIvxPPGb_hb",
        "colab_type": "text"
      },
      "source": [
        "Three wine types are separated well by the first two principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-XTBQvhy0uX",
        "colab_type": "text"
      },
      "source": [
        "# A ton of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LizREaSHy3lL",
        "colab_type": "text"
      },
      "source": [
        "Sklearn has a ton of models that you can use. I will not cover these in detail, but you should be familiar with them from your prior coursework. In particular, you should familiarize yourself with:\n",
        "* K nearest neighbor\n",
        "* Decision trees\n",
        "* Naive bayes\n"
      ]
    }
  ]
}