{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BA865_Lecture_05.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylanwalker/BA865/blob/master/BA865_Lecture_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTfOe5Ymb_Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvfQRYqvb_Z5",
        "colab_type": "text"
      },
      "source": [
        "# scikit-learn: Machine Learning in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVV6S587b_Z_",
        "colab_type": "text"
      },
      "source": [
        "\"scikit\" refers to \"scipy toolkit.\" Numerous machine learning algorithms and models are implemented in [scikit-learn](https://scikit-learn.org/stable/index.html), so you do not need to install additional packages for basic work. \n",
        "\n",
        "Six main categories of the scikit-learn are\n",
        "* Regression\n",
        "* Classification\n",
        "* Clustering\n",
        "* Dimensionality reduction\n",
        "* Model selection\n",
        "* Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nJTFOYlb_aH",
        "colab_type": "text"
      },
      "source": [
        "## Estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3i2SEe6b_aN",
        "colab_type": "text"
      },
      "source": [
        "In scikit-learn, machine learning model is called as **Estimator**.\n",
        "\n",
        "Each **Estimator** is a Python `class` and has a form like (Recall the structure of `class`).\n",
        "\n",
        "```python\n",
        "class estimator():\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def fit():\n",
        "        # do some calculations with self.data\n",
        "```\n",
        "\n",
        "So if you want to estimate coefficients or learn patterns within data, simply do\n",
        "\n",
        "1. initialize an estimator\n",
        "2. Fit the estimator with data of your interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efFIkl6b_aS",
        "colab_type": "text"
      },
      "source": [
        "## Regression : Linear Regression - Ordinary Least Squares (OLS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2TON8pjb_aU",
        "colab_type": "text"
      },
      "source": [
        "Ordinary Least Squares\n",
        "\n",
        "$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p$\n",
        "\n",
        "Estimate $w$ that minimizes $\\sum{(y-Xw)^2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xyPomib_aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize a linear model estimator\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FVlKQ6Yb_aj",
        "colab_type": "text"
      },
      "source": [
        "`LinearRegression` accepts two inputs X and Y.\n",
        "\n",
        "Their formats should be organized as below.\n",
        "\n",
        "X = $[[x_{11}, x_{12}], [x_{21}, x_{22}], [x_{31},x_{32}]]$\n",
        "\n",
        "y = $[y_{1}, y_{2}, y_{3}]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M31wxlV5b_ao",
        "colab_type": "text"
      },
      "source": [
        "In case that $x_i$ has only one value, let $X = [[x_{11}], [x_{21}], [x_{31}]]$.\n",
        "\n",
        "So, if you want to estimate a linear model for the data $X=[1,2,3,4,5]$ and $y=[0,2,4,1,4]$, you should change the format of X as $[[1], [2], [3], [4], [5]]$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pg_t03Fb_ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [0,2,4,1,4]\n",
        "plt.scatter(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz8cak8zb_a0",
        "colab_type": "text"
      },
      "source": [
        "Now fit the estimator `lm`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xihWeA-b_a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imNo2r_eb_a-",
        "colab_type": "text"
      },
      "source": [
        "Estimated intercept $w_0$ and coefficients $w_i$ are stored in `lm.intercept_` and `lm.coef_` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WphayXjb_bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Estimated intercept is', lm.intercept_, 'and estimated coefficient is', lm.coef_[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcba9Obzb_bI",
        "colab_type": "text"
      },
      "source": [
        "The estimated linear model is $Y = 0.1 + 0.7X$.\n",
        "\n",
        "What if you want to predict a value for $x=10$? &rarr; use `lm.predict()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyTqGrdfb_bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm.predict([[10]]) # be sure that X should be given as two dimensional array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o-k1cbub_bT",
        "colab_type": "text"
      },
      "source": [
        "Lastly, using `lm.predict()`, we can also draw the estimated line with data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JHMmMD4b_bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = lm.predict(X)\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, color='gray', linestyle='--')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5idZnEJxb_bd",
        "colab_type": "text"
      },
      "source": [
        "Remember the rules of scikit-learn. Initialize and fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghZFtG5Tb_bf",
        "colab_type": "text"
      },
      "source": [
        "## Classification : Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii6xOeQUb_bj",
        "colab_type": "text"
      },
      "source": [
        "Support vector machine (SVM) is a supervised learning model that classifies data points into given labels.\n",
        "SVM finds hyperplanes that maximally divide labels and uses the hyperplanes as classifiers. \n",
        "\n",
        "* For $p$ dimensional vectors, its hyperplane of $(p-1)$ dimensions can separate the vectors into labels.\n",
        "* For example, if each observation has two values, a hyperplane that divides observations is a line (1-dim).\n",
        "* If each observation has three values, a hyperplane is a plane (2-dim).\n",
        "\n",
        "A hyperplane that divides data points can be expressed as $\\overrightarrow{w}$ that satisfies $\\overrightarrow{w}\\overrightarrow{x}-b = c$ where $c$ is a value between two labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAJhuUDb_bl",
        "colab_type": "text"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RmvBbM3b_bn",
        "colab_type": "text"
      },
      "source": [
        "Randomly generate samples from two multivariate normal distributions. For a data point, its associated distribution is the given label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TYXxWxJb_bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blw0UIeab_bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat1[:,0], dat1[:,1])\n",
        "plt.scatter(dat2[:,0], dat2[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2xLy2Nb_b0",
        "colab_type": "text"
      },
      "source": [
        "1. Initialize an estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKA4iFLnb_b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC() # Linear support vector machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRSb9qudb_b_",
        "colab_type": "text"
      },
      "source": [
        "2. Fit the estimator to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8PrqkZyb_cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((dat1, dat2))\n",
        "y = [0]*50 + [1]*50 # Labels\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoDf07-eb_cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(clf.intercept_, clf.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P78uDG0pb_cS",
        "colab_type": "text"
      },
      "source": [
        "Thus, the learned classifier is $-2.87 + 1.30x_{1}+0.67x_{2}$.\n",
        "\n",
        "Predict labels of other data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv2mxrlbb_cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pred = np.array([[0.5, 0], [1.5, 3], [3, 2]])\n",
        "y_pred = clf.predict(X_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmNBNVDeb_ca",
        "colab_type": "text"
      },
      "source": [
        "#### Check the results on a plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdrs_uMzb_ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat1[:,0], dat1[:,1], alpha=0.2)\n",
        "plt.scatter(dat2[:,0], dat2[:,1], alpha=0.2)\n",
        "\n",
        "X_tmp = np.arange(0.5, 2.5, 0.1) \n",
        "SVM_line = 1/clf.coef_[0][1]*(-clf.intercept_[0] - clf.coef_[0][0]*X_tmp)\n",
        "plt.plot(X_tmp, SVM_line, color='gray', linestyle='--', label='SVM')\n",
        "\n",
        "\n",
        "plt.scatter(X_pred[:,0], X_pred[:,1], marker='s', s=100, \n",
        "            color = ['tab:blue' if x==0 else 'tab:orange' for x in y_pred], label='Predicted')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRExyPGEb_co",
        "colab_type": "text"
      },
      "source": [
        "### Q. Can we build a SVM classifier that predicts who survived the Titanic? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWChR1-Db_cq",
        "colab_type": "text"
      },
      "source": [
        "Load the titanic data from the `seaborn` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK8q7MAwb_ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "titanic=sns.load_dataset('titanic') # load data\n",
        "titanic.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfAaLpMbb_c0",
        "colab_type": "text"
      },
      "source": [
        "While the titanic data consists of many columns, we will use four columns, \"pclass\", \"sex\", \"age\", and \"fare,\" to build a prediction model. Among these selected columns, only the \"sex\" column is \"string.\" We will change \"male\" to 0 and \"female\" to \"1\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMRP7D6ab_c1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = titanic[['pclass', 'sex', 'age', 'fare']] # use four columns\n",
        "X = X.replace({'sex': {'male': 0, 'female': 1}}) # male = 0, female = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiB2nrhBb_c-",
        "colab_type": "text"
      },
      "source": [
        "Some rows that do not have age values are excluded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qT3-Mfqb_dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_na = ~pd.isna(X.age)\n",
        "X = X[not_na]\n",
        "y = titanic['survived'][not_na] # \"survived\" is the label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnKRqWf6b_dH",
        "colab_type": "text"
      },
      "source": [
        "Cleaned data look like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTLQRJ9Vb_dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6h03xHNb_dM",
        "colab_type": "text"
      },
      "source": [
        "Now, initiate a SVM model. Unlike the previous example, we will use the SVC function that provides advanced kernels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsEkAsNsb_dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QB4tdR6b_dT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(X)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2mxsUwZb_dZ",
        "colab_type": "text"
      },
      "source": [
        "## Model selection : How to improve and evaluate the learned model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkw89ZJbb_df",
        "colab_type": "text"
      },
      "source": [
        "Until now, we used the entire data to learn a model. In fact, it is wrong! We need to test the learned model for new data.\n",
        "\n",
        "&rarr; split data into train, validation, and test sets.\n",
        "\n",
        "* Train set: a subset of data to train a model\n",
        "* Validation set: a subset of data to evaluate the model\n",
        "* Test set: a subset of data to evaluate the learned model\n",
        "\n",
        "As our current goal is not to evaluate and build a model of better performance, we will use 70\\% of data as train set and 30\\% as test set.  \n",
        "\n",
        "Fortunately, `sklearn` provides an easy way to split data into train and test sets.\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZf29Os5b_di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Let 30% of the data to be a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgtHuRiPb_dp",
        "colab_type": "text"
      },
      "source": [
        "Now learn a SVM model for the train sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8JaPiFOb_dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='auto', random_state=0) # SVC covers not only linear kernel as LinearSVC but also nonlinear kernels\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGorV5GZb_dx",
        "colab_type": "text"
      },
      "source": [
        "Then predict labels for the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWADQ9Vub_d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDTYKI_zb_d6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_1ae1tob_eD",
        "colab_type": "text"
      },
      "source": [
        "## Clustering : K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hBZWTobb_eF",
        "colab_type": "text"
      },
      "source": [
        "What if no labels are given in data? \n",
        "\n",
        "Based on patterns, computational models make clusters that would work as labels. This process is called **unsupservised learning**. K-means clustering is one of popular unsupervised learning methods.\n",
        "\n",
        "Simple idea: Assume that data points are separated into $K$ clusters. Find $K$ centroids that minimize witin-cluster variances\n",
        "\n",
        "Let $(x_1, x_2, ..., x_n)$ be observations and $(\\mu_1, \\mu_2, ..., \\mu_m)$ be centroids of points in cluster $i, C_i$.\n",
        "\n",
        "Minimize $\\sum_{i=1}^{m}\\sum_{x\\in C_{i}}\\|x-\\mu_i\\|^2$.\n",
        "\n",
        "<span style=\"color:red\"> **The number of clusters $K$ should be given**</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWAnF8gJb_eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The same data that are generated in the SVM example\n",
        "np.random.seed(1)\n",
        "dat1 = np.random.multivariate_normal(mean=[1,1], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat2 = np.random.multivariate_normal(mean=[2,1.5], cov=[[0.3, 0], [0, 0.3]], size=50)\n",
        "dat = np.concatenate((dat1, dat2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdbPoXy3b_eM",
        "colab_type": "text"
      },
      "source": [
        "Assume that we do not know underlying clusters to which data points belong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNoJhDw-b_eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dat[:,0], dat[:,1], color='gray') # but it is originally generated by two different distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozlD2gbHb_eU",
        "colab_type": "text"
      },
      "source": [
        "### K=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZmvDNP4b_eX",
        "colab_type": "text"
      },
      "source": [
        "Set n_clusters=2 in the function KMeans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX3ZYyAlb_eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0) \n",
        "kmeans.fit(dat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Ckk3VWb_eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(dat1[:,0], dat1[:,1])\n",
        "plt.scatter(dat2[:,0], dat2[:,1])\n",
        "plt.title('Original data')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "labels = kmeans.labels_\n",
        "plt.scatter(dat[:,0], dat[:,1], color=['tab:blue' if x==0 else 'tab:orange' for x in labels])\n",
        "plt.title('K-means clustering: K=2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23yzuOBb_en",
        "colab_type": "text"
      },
      "source": [
        "K-means clustering detects similar groups with the original setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCKecUw5b_ep",
        "colab_type": "text"
      },
      "source": [
        "### K=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alAbjo-Mb_es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(dat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQQWOc4Nb_e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(dat1[:,0], dat1[:,1])\n",
        "plt.scatter(dat2[:,0], dat2[:,1])\n",
        "plt.title('Original data')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "labels = kmeans.labels_\n",
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(dat[:,0], dat[:,1], color=[cmap[x] for x in labels])\n",
        "plt.title('K-means clustering: K=4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2YG0rk9b_e_",
        "colab_type": "text"
      },
      "source": [
        "The algorithm classifies data points for a given number of clusters, whatever the number is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPH7_xJb_fB",
        "colab_type": "text"
      },
      "source": [
        "### Example: wine data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5izbjLZb_fE",
        "colab_type": "text"
      },
      "source": [
        "In this data, chemical compositions and types of wines are given. There are three types of wines. We will investigate the degree that k-means clustering recovers wine types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYChin8sb_fG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oijMWQ0Wb_fR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKosMynnb_fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oQWzWjxb_ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_63XiqXob_fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = kmeans.predict(X_test)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXgTmnmMb_fo",
        "colab_type": "text"
      },
      "source": [
        "Not bad, but there is room for improvement. How?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja0Npxxyb_fp",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing : Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh0UA6vqb_ft",
        "colab_type": "text"
      },
      "source": [
        "Standardization is required for data like the wine case because values are distributed with different means and variances by columns. These characteristics would affect performance and implementation of machine learning algorithms. By standardizing, we can resolve this issue to some extent. To standardize data, we will use the `sklearn.preprocessing` package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWgddIab_fu",
        "colab_type": "text"
      },
      "source": [
        "`StandardScaler` calculates mean and standard devaition of a train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge9kAroqb_fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() # calculate mean and standard deviation of train set\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aY3ED_7b_fz",
        "colab_type": "text"
      },
      "source": [
        "Let's print means of columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFlWJanhb_f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler.mean_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSQLQr4Xb_f6",
        "colab_type": "text"
      },
      "source": [
        "\"scaler.scale_\" returns standard deviations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Oshs4FUb_f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler.scale_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDO2KYTb_f-",
        "colab_type": "text"
      },
      "source": [
        "\"scaler.trasform\" standardizes original values. Transformed data look like Gaussian distribution with 0 mean and 1 variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5n5eXaSb_gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJuv-qkXb_gD",
        "colab_type": "text"
      },
      "source": [
        "Let's check means of transformed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OyiMbZb_gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(X_train_scaled, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QKMgBrbb_gI",
        "colab_type": "text"
      },
      "source": [
        "Compared to means of the original data, those of the standardized data are close to zero. Now, train a k-mean clustering model with the standardized data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ7bvzYHb_gJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_train_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5lCdOf5b_gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_scaled = scaler.transform(X_test)\n",
        "y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gqZ2uKtb_gO",
        "colab_type": "text"
      },
      "source": [
        "While we use the same configurations, the ratio of overlapping increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFWjah-Fb_gP",
        "colab_type": "text"
      },
      "source": [
        "### Pipelines : chaining pre-processors and estimators¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0vpFxmb_gP",
        "colab_type": "text"
      },
      "source": [
        "We can do all procedures (standardize data and learn a model) at once by the `sklearn.pipeline` package!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzvouLKib_gQ",
        "colab_type": "text"
      },
      "source": [
        "The below code block shows how to combine \"StandardScaler\" and \"KMeans\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSfp44tb_gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    KMeans(n_clusters=3, random_state=0)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG_FnXyxb_gT",
        "colab_type": "text"
      },
      "source": [
        "To execute this pipe, following the rules of sklearn, use `.fit` method.\n",
        "```python\n",
        "pipe.fit(X_train, Y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFhoHS09b_gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "Y = wine.target # there are three types\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0)\n",
        "\n",
        "pipe.fit(X_train, Y_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "Y_pred = kmeans.predict(X_test_scaled)\n",
        "print('{:.2%}\\n'.format(accuracy_score(Y_test, Y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAJ3WuS4b_ge",
        "colab_type": "text"
      },
      "source": [
        "You can get the same result as before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLsakRUb_gf",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs-21vkrb_gg",
        "colab_type": "text"
      },
      "source": [
        "Through dimensionality reduction, we can get following advantages. (https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
        "* It reduces the time and storage space required.\n",
        "* Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model.\n",
        "* It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n",
        "* It avoids the curse of dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uv3YvAb_gh",
        "colab_type": "text"
      },
      "source": [
        "For example, we can reduce 13 dimensions in the wine data into 2 dimensions. This process helps to understand and visualize complicated data. In this section, we will cover a popular dimensionality reduction method: **principal component analysis (PCA)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbiiqFdb_gi",
        "colab_type": "text"
      },
      "source": [
        "#### What is PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnvWcA0Tb_gj",
        "colab_type": "text"
      },
      "source": [
        "PCA finds linearly uncorrelated variables by combining existing correlated variables. Let's scratch the concept from the wine data. In the wine data, \"alcohol\" and \"color_intensity\" are correlated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvHMReNUb_gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "wine = load_wine() \n",
        "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
        "y = wine.target # there are three types\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "X_train_scaled = scaler.transform(X_train) # You can apply the scaler even to test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYl8rU5b_go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,-4])\n",
        "plt.xlabel(\"Alcohol (standardized)\")\n",
        "plt.ylabel(\"Color intensity (standardized)\")\n",
        "print(\"Correlation between alchol and color intensity is\", round(X.alcohol.corr(X.color_intensity), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ABrRX9b_gs",
        "colab_type": "text"
      },
      "source": [
        "As the two variables are correlated, significant amount of variances between them can be captured through a new variable. PCA returns this new variable by combining correlated ones. The new variable is represented as the arrow on the below plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fx4Rkxb_gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled[:, (0,-4)]) # find principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K_q_VDYb_g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,-4], alpha=0.3)\n",
        "plt.xlabel(\"Alcohol (standardized)\")\n",
        "plt.ylabel(\"Color intensity (standardized)\")\n",
        "plt.annotate(\"\", [0,0], -3*pca.explained_variance_ratio_[0]*pca.components_[:,0], \n",
        "             arrowprops=dict(arrowstyle='<-', linewidth=3, color='red'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koeqZmKCb_g9",
        "colab_type": "text"
      },
      "source": [
        "In this way, PCA finds a given number of components (= n_components) that are uncorrelated and explain variances well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-MalDzcb_g-",
        "colab_type": "text"
      },
      "source": [
        "\"pca.explained_variance_ratio_\" summarizes how much variance that a component explains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-QtCkxob_hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ebLePtb_hC",
        "colab_type": "text"
      },
      "source": [
        "It shows that the first principal component (red arrow) explains about 78% of the total variances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBTMrmXIb_hD",
        "colab_type": "text"
      },
      "source": [
        "Let's apply PCA for the entire wine data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ujm6qzb_hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, random_state=0) # 13 dimensions to 2 dimensions\n",
        "pca.fit(X_train_scaled) # find principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-IwUWDhb_hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VklOxnlb_hO",
        "colab_type": "text"
      },
      "source": [
        "About 37\\% variances are explained by the first principle component and about 19\\% variances are explained by the second principle component. It means that the first two components capture more than half of all variances. \n",
        "\n",
        "So, projecting the wine data onto the first two principal components can give a good overview of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h63-M3a-b_hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca_transformed = pca.transform(X_train_scaled) # project the data onto principal components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MSsVmNQb_hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmap = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
        "plt.scatter(pca_transformed[:,0], pca_transformed[:,1], color = [cmap[x] for x in y_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKIvxPPGb_hb",
        "colab_type": "text"
      },
      "source": [
        "Three wine types are separated well by the first two principal components"
      ]
    }
  ]
}